---
title: Attention
date: 2022-10-07 00:00
status: draft
modified: 2023-04-16 00:00
---

Attention is an approach for [Encoder-Decoder](encoder-decoder.md) architectures, where we pass all hidden states from the encoder, instead of a final representation.

The idea was first published in [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) by Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio and is a key component of the [Transformer](transformer.md) architecture.
