---
title: Thought Policy Optimisation
date: 2024-10-16 00:00
modified: 2024-10-16 00:00
status: draft
---

A fine-tuning technique based on **Direct Preference Optimisation (DPO)**, uses a Judge model to evaluate model outputs based solely on the responses, without access to the thought process; this lets the model learn and refine its "thinking" abilities without relying on supervised thought data.

See [Thinking LLMs: General Instruction Following with Thought Generation (Oct 2024)](../reference/papers/thinking-llms-general-instruction-following-with-thought-generation.md).
