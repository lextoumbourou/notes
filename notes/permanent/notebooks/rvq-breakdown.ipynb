{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfea930b-3dd5-424e-a7d6-79f3fa7f4160",
   "metadata": {},
   "source": [
    "# RVQ Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea90dafc-ccc4-4ee7-b97f-6e0f38b68855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd84d8b-619e-4b49-b9ed-6299f8aef0dd",
   "metadata": {},
   "source": [
    "## Input\n",
    "\n",
    "1 second audio file at a 44.1kHz sample rate.\n",
    "\n",
    "Batch size of 1 (first dim), 1 channel mono audio (second dim) and exactly 2 seconds of audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624527cf-86d9-462a-a7ff-dc6ad3438b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "channels = 1\n",
    "audio_length = 1 * 44100\n",
    "audio_batch = torch.randn(batch_size, channels, audio_length).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a85629-705f-4ebd-9996-4479fbb0558f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 44100])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdb3e23-0680-40b5-a8ea-5a139cbd6f10",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf7a7f1-87dd-495e-a116-8c40a2f01e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import weight_norm\n",
    "import torch.nn as nn\n",
    "from dac.nn.layers import Snake1d\n",
    "\n",
    "WNConv1d = lambda *args, **kwargs: weight_norm(nn.Conv1d(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d417067-0c35-45f5-9849-3bd73e914197",
   "metadata": {},
   "source": [
    "First convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39de8b90-1e2e-4b72-9e03-a133044f7839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lex/code/private-notes/public/notes/permanent/notebooks/.env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "d_model = 64\n",
    "block1 = WNConv1d(1, d_model, kernel_size=7, padding=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3715c76-f179-4cb2-a4f7-107e19adde45",
   "metadata": {},
   "outputs": [],
   "source": [
    "block1_out = block1(audio_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceb1e6c0-62c5-48be-b2c5-3ec1c959bf81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 44100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block1_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f7c8f-cee1-4d88-ae31-8b970b652de4",
   "metadata": {},
   "source": [
    "Encoder blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4153749-cb32-4560-8ecd-88b8e99c053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(nn.Module):\n",
    "    def __init__(self, dim: int = 16, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        pad = ((7 - 1) * dilation) // 2\n",
    "        self.block = nn.Sequential(\n",
    "            Snake1d(dim),\n",
    "            WNConv1d(dim, dim, kernel_size=7, dilation=dilation, padding=pad),\n",
    "            Snake1d(dim),\n",
    "            WNConv1d(dim, dim, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.block(x)\n",
    "        pad = (x.shape[-1] - y.shape[-1]) // 2\n",
    "        if pad > 0:\n",
    "            x = x[..., pad:-pad]\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dim: int = 16, stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ResidualUnit(dim // 2, dilation=1),\n",
    "            ResidualUnit(dim // 2, dilation=3),\n",
    "            ResidualUnit(dim // 2, dilation=9),\n",
    "            Snake1d(dim // 2),\n",
    "            WNConv1d(\n",
    "                dim // 2,\n",
    "                dim,\n",
    "                kernel_size=2 * stride,\n",
    "                stride=stride,\n",
    "                padding=math.ceil(stride / 2),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d529f67-f397-44a3-b620-0df9fe97e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encblock1 = EncoderBlock(d_model*2, stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162074f-eb7d-47ee-8ca2-41586f65abf6",
   "metadata": {},
   "source": [
    "Audio is downsampled, while number of channels is doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96d1e428-5fdf-4230-a76e-a16c0db60f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc1out = encblock1(block1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed9a5378-9dda-40ca-8bed-822ba4fbe1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 22050])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc1out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66526aa3-f64a-498a-aca0-f6ee4c69981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encblock2 = EncoderBlock(d_model*2*2, stride=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ce8afeb-445f-47a5-b793-083223ac71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc2out = encblock2(enc1out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cbd3a74-15df-42d4-b22b-472a711922b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 5512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc2out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "453f20d5-384f-4c5d-84d6-6e821650a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "encblock3 = EncoderBlock(d_model*2*2*2, stride=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a26e505a-0b42-4f63-a587-2a8b0fc5c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "encblock4 = EncoderBlock(d_model*2*2*2*2, stride=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9abf8b1b-679c-49e4-bf5f-67691a744a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 689])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = encblock3(enc2out)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81330904-3472-4827-a829-58200d27d59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 86])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = encblock4(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a6d3699-9dfe-4840-804f-c6aae3b0f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lastact = Snake1d(d_model*2*2*2*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7610773e-1ebd-4108-a4ed-14ec499a4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lastact(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e43614a0-e667-4fcb-9b47-83fcad359f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 86])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980582f-8100-49a8-87ec-c85a5364eef9",
   "metadata": {},
   "source": [
    "Last conv takes that downsampled collection and projects back into the latnt size of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10fcba63-bcd6-4052-bfea-88c4cf3024b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encout = WNConv1d(d_model*2*2*2*2, 64, kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3d57f61-173a-411e-92b7-81611142dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = encout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aeb9b5c5-7b39-437a-909f-a543ad75be56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 86])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed025c08-3bac-4e0e-8c09-2491ea66d1fa",
   "metadata": {},
   "source": [
    "So now we have an encoding of the audio. For 1 seconds of audio, we have 86 frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90fb33-8b87-4fc3-829d-7f448e1ab575",
   "metadata": {},
   "source": [
    "## Vector Quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d46f24-6023-4ff2-afe6-44d54785541f",
   "metadata": {},
   "source": [
    "Let's start by exploring basic vector quantising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3819bc6d-109e-4b93-a096-0ef7e76aef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dac.nn.quantize import VectorQuantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf83d9e5-a125-4f43-aeb5-efe92ef08005",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_quantize = VectorQuantize(64, 1024, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12994fb5-d505-4ec4-8a38-727249048df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 8])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_quantize.codebook.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aaa57e25-26c0-4a28-b467-8031cb804eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_q, commitment_loss, codebook_loss, indices, z_e = vec_quantize(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6279e36-70a6-4a30-bff2-3efa9cb29611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 86])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beb2c435-c762-4345-b43a-6e1137ae00dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[326, 939, 428, 993, 557, 240, 745, 118, 468, 428,  78, 847, 444, 468,\n",
       "         542, 660,  23, 428, 224, 993, 110, 548, 727, 212, 428, 847, 418, 750,\n",
       "         280, 993, 685, 589, 550, 468, 479, 260, 770, 321, 619, 224, 260, 877,\n",
       "         939, 428, 446, 260, 869, 554, 260, 849, 855,  26, 260, 848, 524, 847,\n",
       "         997, 908, 226, 750, 826, 670, 847,  26, 554, 555, 750,  61,  36, 939,\n",
       "         727, 316, 325, 849, 118,  89, 295, 595, 446, 555, 847, 997, 769, 212,\n",
       "         428, 914]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642714d-e47e-4f8a-954d-c866d956b5d2",
   "metadata": {},
   "source": [
    "## Residual Vector Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e69744d0-db05-4957-836d-1aa0cf4d1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dac.nn.quantize import ResidualVectorQuantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7175ea2d-250d-496c-9889-b630d5503e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = ResidualVectorQuantize(\n",
    "    input_dim=64,\n",
    "    n_codebooks=9,\n",
    "    codebook_dim=8,\n",
    "    quantizer_dropout=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a0f44-55f8-4647-9a55-a53a90cae292",
   "metadata": {},
   "source": [
    "From the quantizer, we get 5 outputs:\n",
    "- z: quantized continuous representation of input\n",
    "- codes: codebook indicies for each codebook - this is the quantized discrete representation.\n",
    "- latents: projected latents (continuous representation of input before quantization.\n",
    "- commitment_loss - committment loss to train encoder to predict vectors closer to codebook.\n",
    "- codebook_loss - codebook loss to update the codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34ae8f9a-c6da-4e88-8bf1-9ed729dfa3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z, codes, latents, commitment_loss, codebook_loss = quantizer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c6c8181-d540-4f46-888b-5743db75fa6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 172])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7e1f2d2-e3ec-495a-9061-44ee0ac86f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 172])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f65fd104-c9ee-4673-bc02-0d674d936b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 101,  764,  495,  507,  187,  541,  661,  115,  187,  187,  708,\n",
       "           494,  115,  187,  494,  928,  381,  639,  494,  541,  541,  516,\n",
       "           928,  354,  115,  187,  494,  507,  106,  187,  708,  708,  661,\n",
       "           629,  495,  740,  507,   19,  774,  928,  393,  115,  789,  347,\n",
       "           740,  494,  115,  928,  886,  928,  187,  101,   46,  187,  187,\n",
       "            68,  928,  642,  928,  393,  507,  187,  740,  187,  557,  187,\n",
       "           507,  784,  502,   19,  187,  187,  187,  719,  834,  187,  106,\n",
       "           187,  784,  928,  546,   46,  129,  507,  541,  347,  295,  928,\n",
       "            46,  187,  494,  507,  516,  115,  708,  740,  347,  115,  115,\n",
       "           187,  642,  766,  187,  115,  187,  928,  928,  928,  494,   97,\n",
       "           834,  987,  187,  541,  494,  928,  912,  642,  101,  129,  642,\n",
       "            46,  187,  784,  115,  642,  494,  115,  495,  240,  354,  507,\n",
       "           347,  495,  507,  886,  494,  347,  600,  187,  494,  507,  187,\n",
       "           987,  541,  785, 1008,  140,  148,  494,  129,  928,  101,  882,\n",
       "           297,  347,  541,   18,  642,  494,  106,  115,  115,  774,  106,\n",
       "           784,  240,  101,  728,  187,  347,  101]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes[:,[0],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "629602ba-3f23-4a1e-bad2-17dee3d6e93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 72, 172])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b64c3084-fd0f-4842-8de9-1f07acafb10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.7737, grad_fn=<AddBackward0>),\n",
       " tensor(3.7737, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commitment_loss, codebook_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1fc9b-ca14-4bf5-a3c2-7016c9c24f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
