{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib seaborn datasets transformers tqdm  tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41734b-09b4-4f7b-b1c1-c4976e0ed422",
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73aabcc-7a9c-4b28-a55c-fed53c8365b2",
   "metadata": {},
   "source": [
    "## Reproduce \"Neural Machine Translation by Jointly Learning to Align and Translate\"\n",
    "\n",
    "In this notebook, I attempt to reproduce the results from the paper [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) by implementing RNNSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "81c79d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenizers.models import WordPiece\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24ed6e-8de1-4241-8bda-55e8651bdb29",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c487a-6384-421b-b31e-de070d4ed063",
   "metadata": {},
   "source": [
    "The paper demonstrates the approach on an English to French translation task, using the data provided as part of the [Workshop on Statistical Machine Translation in 2014](https://aclanthology.org/W14-3302.pdf). I've found a version of that on HuggingFace. Not sure exactly how closely it mirrors the paper, but I'm not too concerned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9af9d7e0-9d65-4816-a875-6608c3f35d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a55aaa2bae47f9b6476194f32718ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1192718d4e754a04a0f3e54dbe697d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11aa87d6643146b5b9954ad041f8d267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'fr'],\n",
      "        num_rows: 40836876\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['en', 'fr'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'fr'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"presencesw/wmt14_fr_en\")\n",
    "print(\"Dataset structure:\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec87c9-5052-4519-b6a2-a3250a577a7c",
   "metadata": {},
   "source": [
    "In the paper, they \"concat news-test-2012 and news-test-2013\" for the validation set, but I'm using the validation set kindly provided by presencesw."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341f46d-6879-4179-b166-05cbfb62f55e",
   "metadata": {},
   "source": [
    "## Tokeniser\n",
    "\n",
    "They use the tokenisation script from open-source package Moses.\n",
    "\n",
    "A Python wrapper exists called `pip install mosestokenizer` which I've installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5b1526e3-061d-43c7-ac4d-c3b7bf2f99ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'In his briefing on economic development, Al Horner will give you details of programs we fund to foster partnerships between the private sector and First Nations and Inuit communities, in areas like resource development projects, for example.',\n",
       " 'fr': \"Dans sa présentation sur le développement économique, M. Al Horner vous donnera des détails sur les programmes que nous finançons pour favoriser l'établissement de partenariats entre le secteur privé et les collectivités des Premières nations et inuites dans des domaines comme celui de l'exploitation des ressources naturelles.\"}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7b56f202-bb3c-4961-b870-5bcc1b7cc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0401b15e-6988-4593-b5a1-dd59db43e4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250027"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0b681a5a-9d3b-4086-a537-42c848c5dcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'his', 'brief', 'ing', 'on', 'economic', 'development', ',', 'Al', 'Horn', 'er', 'will', 'give', 'you', 'details', 'of', 'programs', 'we', 'fund', 'to', 'fost', 'er', 'partnership', 's', 'between', 'the', 'private', 'sector', 'and', 'First', 'Nations', 'and', 'In', 'uit', 'communities', ',', 'in', 'areas', 'like', 'resource', 'development', 'projects', ',', 'for', 'example', '.', '</s>', 'en_XX']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode(t) for t in tokenizer(example[\"en\"])[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fdaefcb2-5005-4ef7-9d18-13f1943c5887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dans', 'sa', 'présentation', 'sur', 'le', 'développement', 'économique', ',', 'M', '.', 'Al', 'Horn', 'er', 'vous', 'donner', 'a', 'des', 'détails', 'sur', 'les', 'programme', 's', 'que', 'nous', 'fina', 'nç', 'ons', 'pour', 'favoriser', 'l', \"'\", 'établissement', 'de', 'partenariat', 's', 'entre', 'le', 'secteur', 'privé', 'et', 'les', 'collectivités', 'des', 'Premi', 'ères', 'na', 'tions', 'et', 'in', 'uit', 'es', 'dans', 'des', 'domaine', 's', 'comme', 'celui', 'de', 'l', \"'\", 'exploitation', 'des', 'ressources', 'naturelle', 's', '.', '</s>', 'en_XX']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode(t) for t in tokenizer(example[\"fr\"])[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ad886-5092-4d56-9049-d247faa82172",
   "metadata": {},
   "source": [
    "From the paper:\n",
    "\n",
    "> After a usual tokenization, we use a shortlist of 30,000 most frequent words in each language to train our models.\n",
    "> Any word not included in the shortlist is mapped to a special token ([UNK]).\n",
    "> We do not apply any other special preprocessing, such as lowercasing or stemming, to the data.\n",
    "\n",
    "To achieve that, I'll create a counter of words. Then we'll cull anything that falls out of the most frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d71a9-e074-4bde-aaf6-4b12c38fb205",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We train two types of models.\n",
    "\n",
    "The first one is an RNN Encoder–Decoder (RNNencdec, Cho et al., 2014a), and the other is the proposed model, to which we refer as RNNsearch.\n",
    "\n",
    "We train each model twice: first with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then with the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50).\n",
    "\n",
    "The encoder and decoder of the RNNencdec have 1000 hidden units each.\n",
    "\n",
    "The encoder of the RNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000 hidden units. Its decoder has 1000 hidden units.\n",
    "\n",
    "\n",
    "We use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model.\n",
    "\n",
    "Each SGD update direction is computed using a minibatch of 80 sentences. We trained each model for approximately 5 days.\n",
    "\n",
    "Once a model is trained, we use a beam search to find a translation that approximately maximizes the conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al., 2013). Sutskever et al. (2014) used this approach to generate translations from their neural machine translation model. For more details on the architectures of the models and training procedure used in the experiments, see Appendices A and B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db64e3-6d62-4d6b-80a3-b2a54b72391e",
   "metadata": {},
   "source": [
    "### Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f3b32-5135-4cc7-9131-0157d34d3665",
   "metadata": {},
   "source": [
    "For all the models used in this paper, the size of a hidden layer $n$ is 1000, the word embedding dimensionality $m$ is 620 and the size of the maxout hidden layer in the deep output $l$ is 500. The number of hidden units in the alignment model $n'$ is 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "92b13677-b75b-41cd-8231-9b70aff622fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 620\n",
    "hidden_size = 1000\n",
    "maxout_size = 500\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "42bc840f-bc4c-48b4-8b40-fb5895e43837",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxout_layer = MaxoutLayer(hidden_size + embed_size, maxout_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "93485ccb-5212-4f26-96af-114ed36c6675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxout_layer(torch.rand(1, 1620)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c0223-3ec2-48de-b13c-0e8a448e4d58",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "In the paper, they use a Bi-Directional RNN, which has a forward RNN and a backward RNN.\n",
    "\n",
    "Effectively, we have an RNN that operates on the normal sequence, and another on the reversed.\n",
    "\n",
    "Each token is then concanted togetether for so that it has context from behind and forwards.\n",
    "\n",
    "That gives us a bidirectional context vector for each word.\n",
    "\n",
    "The Pytorch RNN function already has bidrecitonal capability. It return the encoding in both directions, and we just simple concat the values together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b4ef60f7-e316-493d-a1e7-abe81a29f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor([ [0,1,2,3] ]).long() # Batch x Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e3f46ee0-86e3-4240-af01-5b3472ef82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "encoder = nn.GRU(embed_size, hidden_size, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "543307f3-116f-436d-b04d-b4593f7e34b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 620])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = encoder_embedding(token_ids)  # Batch x Sequence x Embedding Dimension\n",
    "embedding.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "74032bae-cc60-416a-9d76-64173861752b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2000])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out, hidden = encoder(embedding)\n",
    "encoder_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2bece7bf-4592-44b3-b9c5-bbb816258a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1000])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71869d1-6d80-4f4a-b5f4-04f69157505c",
   "metadata": {},
   "source": [
    "From https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "\n",
    "For bidirectional GRUs, forward and backward are directions 0 and 1 respectively.\n",
    "\n",
    "In the paper, the decoder initial hidden state uses the last hidden state from the encoder. Which is the first state in the backwards pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e40f72b8-d349-4da3-9e60-ac6bbb64bb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = hidden[1][0:]\n",
    "hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d4c0c-3f5a-48f3-a315-c05d85f64fef",
   "metadata": {},
   "source": [
    "Here I wrap all into a PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3d3afa38-e9b7-43bb-863d-1ac3196ad391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        hidden = hidden[1][0:]\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fa313d8f-40ed-4215-b022-48d560329a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 2000]), torch.Size([1, 1000]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(vocab_size, embed_size, hidden_size)\n",
    "encoder_outputs, encoder_hidden = enc(token_ids)\n",
    "encoder_outputs.shape, encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a99eb-4039-44ed-bca8-f3a4f959fd09",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d38bfb-e65b-434d-9609-3e7ffbc36a38",
   "metadata": {},
   "source": [
    "The initial hidden state $s_0$ is computed by $s_0 = \\tanh \\left( W_s \\overleftarrow{h}_1 \\right)$ where $W_s \\in \\mathbb{R}^{n \\times n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "14974219-85f6-4ea6-9a94-65659ead6c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1000])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_state = nn.Linear(hidden_size, hidden_size)\n",
    "dec_hidden = torch.tanh(init_state(encoder_hidden)).unsqueeze(0)\n",
    "dec_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5270406-bdae-4f0a-87e3-2ff19a1d565f",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239ca8a-97e0-4819-be23-d4e78e7e24bc",
   "metadata": {},
   "source": [
    "For the encoder outputs, we do a linearation project into the context size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5fb74d57-4eef-4c58-abd7-bf947f83a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_context = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "# As per the paper, we only use one direction of final hidden state. So doesn't need to be doubled.\n",
    "attention_hidden = nn.Linear(hidden_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "11965bc4-4c6b-426b-b3a1-ad450bb0bdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1000])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_proj = attention_context(outputs)\n",
    "context_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "086728b1-29e6-40f2-96ab-c0e290401861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1000])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_proj = attention_hidden(dec_hidden)\n",
    "hidden_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "43da00ec-ce7d-43d8-af5d-876394076cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1000])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_vector = torch.tanh(hidden_proj + context_proj)\n",
    "attention_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "555b8083-6733-4522-959a-616427daaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_alignment = nn.Linear(hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6f0d16b9-04a8-4bf9-aa7b-87660f67fb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = attention_alignment(attention_vector)\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4bf42bdb-00e2-4e5d-83c6-7ff15550521a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = attention_alignment(attention_vector).squeeze(2)\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0b03bff9-dc1e-4686-aee7-742c8534e82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1133, 0.1081, 0.0164, 0.0054]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d345370-6d20-45ee-b490-ce95b3c52647",
   "metadata": {},
   "source": [
    "Then a projection of the decoder hidden.\n",
    "\n",
    "Finally a Softmax is performed to convert into a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "eb11d7f4-111b-4776-9127-707100ce5732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2632, 0.2618, 0.2388, 0.2362]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = F.softmax(attention_scores, dim=1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "42a4936c-2074-4fbd-b3f5-49ebd1d461e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f795e-4860-44a9-ba39-d3e42ea3b96d",
   "metadata": {},
   "source": [
    "And now we do a matrix multiplication operation, which gives us the final weighed sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7a439b88-7c00-4751-a3ad-09dcfeb433fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2000])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.bmm(attention_weights.unsqueeze(1), outputs)\n",
    "context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e7197b69-3814-41fd-b3ad-81d03b85eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention_context = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.attention_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attention_alignment = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, decoder_state, encoder_outputs, attention_mask=None):\n",
    "        hidden_projection = self.attention_hidden(decoder_state)\n",
    "        context_projection = self.attention_context(encoder_outputs)\n",
    "        \n",
    "        attention_vector = torch.tanh(hidden_projection + context_projection)\n",
    "        attention_scores = self.attention_alignment(attention_vector).squeeze(2)\n",
    "        \n",
    "        # Apply attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(\n",
    "                ~attention_mask.bool(), \n",
    "                float('-inf')\n",
    "            )\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        \n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5c455d88-2898-4011-ae5a-f854a265bd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2000])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = Attention(hidden_size)\n",
    "context, attention_weights = attn(dec_hidden, encoder_out)\n",
    "context.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973463ac-1cce-4e7e-b0a2-eaee3e535d43",
   "metadata": {},
   "source": [
    "### Maxout Layer\n",
    "\n",
    "In both cases, we use a multilayer network with a single maxout (Goodfellow et al., 2013) hidden layer to compute the conditional probability of each target word (Pascanu et al., 2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee12aa-d5dd-4b1e-b848-300f5f180448",
   "metadata": {},
   "source": [
    "### Maxout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b5503-2c4d-4553-96c5-414c6bfba653",
   "metadata": {},
   "source": [
    "The final layer of the decoder is a Maxout layer, which projects a linear layer into two buckets and takes the max. A form of regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d25b63d9-5eb0-4ac5-a374-f93147e6b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxoutLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_pieces=2):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_pieces = num_pieces\n",
    "        self.linear = nn.Linear(in_features, out_features * num_pieces)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shape = [x.shape[0], self.out_features, self.num_pieces]\n",
    "        # Project into num_features * out_features\n",
    "        x = self.linear(x) # B, F\n",
    "        # Project into (num_features, out_features, num_pieces)\n",
    "        x = x.view(*shape)  # B, F, P\n",
    "        # Take the max, which should take the match out of either bucket.\n",
    "        x, _ = torch.max(x, -1) # B, F\n",
    "        return x # B, F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807de9f-8d84-406b-ad8c-c6efe9b65b58",
   "metadata": {},
   "source": [
    "### Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7da9e086-88e9-4caf-ad7f-6bc82d70c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        \n",
    "        # Input size is embedding + context vector\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_dim + hidden_dim * 2,\n",
    "            hidden_dim * 2,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.output = MaxoutLayer(hidden_dim * 2, vocab_size)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, attention_mask):\n",
    "        # input shape: [batch_size, 1]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # Get context vector from attention\n",
    "        context, attention_weights = self.attention(\n",
    "            hidden.unsqueeze(1), encoder_outputs, attention_mask\n",
    "        )\n",
    "        \n",
    "        # Combine embedding and context vector\n",
    "        rnn_input = torch.cat([embedded, context], dim=2)\n",
    "\n",
    "        # Pass through RNN\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        # Generate output distribution\n",
    "        prediction = self.output(output.squeeze(1))\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "597f4a81-745d-45c1-80f8-b5c49f371b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_batch(batch_size=32, max_src_len=20, max_tgt_len=20, \n",
    "                       src_vocab_size=1000, tgt_vocab_size=1000):\n",
    "    src = torch.randint(0, src_vocab_size, (batch_size, max_src_len))\n",
    "    tgt = torch.randint(0, tgt_vocab_size, (batch_size, max_tgt_len))\n",
    "    src_lengths = torch.randint(5, max_src_len + 1, (batch_size,))\n",
    "    return src, tgt, src_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf58c65-0760-4814-94d2-eb889e85fa0b",
   "metadata": {},
   "source": [
    "Let's create some sample batches firstly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "31702fef-5fbd-4881-9dd3-56c776ef1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "max_src_len=20\n",
    "max_tgt_len=20\n",
    "src_vocab_size=1000\n",
    "tgt_vocab_size=1000\n",
    "\n",
    "source = torch.randint(0, src_vocab_size, (batch_size, max_src_len))\n",
    "target = torch.randint(0, tgt_vocab_size, (batch_size, max_tgt_len))\n",
    "source_lengths = torch.randint(5, max_src_len + 1, (batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1d5f5921-4f62-4236-8a6f-6e9097c27e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "cf5147d4-cc8c-45f5-822e-71129b1d542f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5d32f7a1-dfd1-4963-86bf-277152d0800c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_lengths.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
