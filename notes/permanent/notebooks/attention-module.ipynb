{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib seaborn datasets transformers tqdm  tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41734b-09b4-4f7b-b1c1-c4976e0ed422",
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73aabcc-7a9c-4b28-a55c-fed53c8365b2",
   "metadata": {},
   "source": [
    "## Reproduce \"Neural Machine Translation by Jointly Learning to Align and Translate\"\n",
    "\n",
    "In this notebook, I attempt to reproduce the results from the paper [Neural Machine Translation by Jointly Learning to Align and Translate] by implementing RNNSearch.\n",
    "\n",
    "I first explore it step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "81c79d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenizers.models import WordPiece\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24ed6e-8de1-4241-8bda-55e8651bdb29",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c487a-6384-421b-b31e-de070d4ed063",
   "metadata": {},
   "source": [
    "The paper demonstrates the approach on an English to French translation task, using the data provided as part of the [Workshop on Statistical Machine Translation in 2014](https://aclanthology.org/W14-3302.pdf). I've found a version of that on HuggingFace. Not sure exactly how closely it mirrors the paper, but I'm not too concerned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9af9d7e0-9d65-4816-a875-6608c3f35d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a55aaa2bae47f9b6476194f32718ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1192718d4e754a04a0f3e54dbe697d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11aa87d6643146b5b9954ad041f8d267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'fr'],\n",
      "        num_rows: 40836876\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['en', 'fr'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'fr'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"presencesw/wmt14_fr_en\")\n",
    "print(\"Dataset structure:\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec87c9-5052-4519-b6a2-a3250a577a7c",
   "metadata": {},
   "source": [
    "In the paper, they \"concat news-test-2012 and news-test-2013\" for the validation set, but I'm using the validation set kindly provided by presencesw."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341f46d-6879-4179-b166-05cbfb62f55e",
   "metadata": {},
   "source": [
    "## Tokeniser\n",
    "\n",
    "They use the tokenisation script from open-source package Moses.\n",
    "\n",
    "A Python wrapper exists called `pip install mosestokenizer` which I've installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5b1526e3-061d-43c7-ac4d-c3b7bf2f99ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'In his briefing on economic development, Al Horner will give you details of programs we fund to foster partnerships between the private sector and First Nations and Inuit communities, in areas like resource development projects, for example.',\n",
       " 'fr': \"Dans sa présentation sur le développement économique, M. Al Horner vous donnera des détails sur les programmes que nous finançons pour favoriser l'établissement de partenariats entre le secteur privé et les collectivités des Premières nations et inuites dans des domaines comme celui de l'exploitation des ressources naturelles.\"}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7b56f202-bb3c-4961-b870-5bcc1b7cc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0401b15e-6988-4593-b5a1-dd59db43e4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250027"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0b681a5a-9d3b-4086-a537-42c848c5dcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'his', 'brief', 'ing', 'on', 'economic', 'development', ',', 'Al', 'Horn', 'er', 'will', 'give', 'you', 'details', 'of', 'programs', 'we', 'fund', 'to', 'fost', 'er', 'partnership', 's', 'between', 'the', 'private', 'sector', 'and', 'First', 'Nations', 'and', 'In', 'uit', 'communities', ',', 'in', 'areas', 'like', 'resource', 'development', 'projects', ',', 'for', 'example', '.', '</s>', 'en_XX']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode(t) for t in tokenizer(example[\"en\"])[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fdaefcb2-5005-4ef7-9d18-13f1943c5887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dans', 'sa', 'présentation', 'sur', 'le', 'développement', 'économique', ',', 'M', '.', 'Al', 'Horn', 'er', 'vous', 'donner', 'a', 'des', 'détails', 'sur', 'les', 'programme', 's', 'que', 'nous', 'fina', 'nç', 'ons', 'pour', 'favoriser', 'l', \"'\", 'établissement', 'de', 'partenariat', 's', 'entre', 'le', 'secteur', 'privé', 'et', 'les', 'collectivités', 'des', 'Premi', 'ères', 'na', 'tions', 'et', 'in', 'uit', 'es', 'dans', 'des', 'domaine', 's', 'comme', 'celui', 'de', 'l', \"'\", 'exploitation', 'des', 'ressources', 'naturelle', 's', '.', '</s>', 'en_XX']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode(t) for t in tokenizer(example[\"fr\"])[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ad886-5092-4d56-9049-d247faa82172",
   "metadata": {},
   "source": [
    "From the paper:\n",
    "\n",
    "> After a usual tokenization, we use a shortlist of 30,000 most frequent words in each language to train our models.\n",
    "> Any word not included in the shortlist is mapped to a special token ([UNK]).\n",
    "> We do not apply any other special preprocessing, such as lowercasing or stemming, to the data.\n",
    "\n",
    "To achieve that, I'll create a counter of words. Then we'll cull anything that falls out of the most frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d71a9-e074-4bde-aaf6-4b12c38fb205",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We train two types of models.\n",
    "\n",
    "The first one is an RNN Encoder–Decoder (RNNencdec, Cho et al., 2014a), and the other is the proposed model, to which we refer as RNNsearch.\n",
    "\n",
    "We train each model twice: first with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then with the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50).\n",
    "\n",
    "The encoder and decoder of the RNNencdec have 1000 hidden units each.\n",
    "\n",
    "The encoder of the RNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000 hidden units. Its decoder has 1000 hidden units.\n",
    "\n",
    "\n",
    "We use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model.\n",
    "\n",
    "Each SGD update direction is computed using a minibatch of 80 sentences. We trained each model for approximately 5 days.\n",
    "\n",
    "Once a model is trained, we use a beam search to find a translation that approximately maximizes the conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al., 2013). Sutskever et al. (2014) used this approach to generate translations from their neural machine translation model. For more details on the architectures of the models and training procedure used in the experiments, see Appendices A and B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db64e3-6d62-4d6b-80a3-b2a54b72391e",
   "metadata": {},
   "source": [
    "### Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f3b32-5135-4cc7-9131-0157d34d3665",
   "metadata": {},
   "source": [
    "For all the models used in this paper, the size of a hidden layer $n$ is 1000, the word embedding dimensionality $m$ is 620 and the size of the maxout hidden layer in the deep output $l$ is 500. The number of hidden units in the alignment model $n'$ is 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "92b13677-b75b-41cd-8231-9b70aff622fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 620\n",
    "hidden_size = 1000\n",
    "maxout_size = 500\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76975a8f-8a0c-4667-b2e6-ddc444e1a5f5",
   "metadata": {},
   "source": [
    "### Maxout Layer\n",
    "\n",
    "In both cases, we use a multilayer network with a single maxout (Goodfellow et al., 2013) hidden layer to compute the conditional probability of each target word (Pascanu et al., 2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d25b63d9-5eb0-4ac5-a374-f93147e6b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxoutLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_pieces=2):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_pieces = num_pieces\n",
    "        self.linear = nn.Linear(in_features, out_features * num_pieces)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shape = [x.shape[0], self.out_features, self.num_pieces]\n",
    "        # Project into num_features * out_features\n",
    "        x = self.linear(x) # B, F\n",
    "        # Project into (num_features, out_features, num_pieces)\n",
    "        x = x.view(*shape)  # B, F, P\n",
    "        # Take the max, which should take the match out of either bucket.\n",
    "        x, _ = torch.max(x, -1) # B, F\n",
    "        return x # B, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "42bc840f-bc4c-48b4-8b40-fb5895e43837",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxout_layer = MaxoutLayer(hidden_size + embed_size, maxout_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "93485ccb-5212-4f26-96af-114ed36c6675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxout_layer(torch.rand(1, 1620)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c0223-3ec2-48de-b13c-0e8a448e4d58",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "In the paper, they use a Bi-Directional RNN, which has a forward RNN and a backward RNN.\n",
    "\n",
    "Effectively, we have an RNN that operates on the normal sequence, and another on the reversed.\n",
    "\n",
    "Each token is then concanted togetether for so that it has context from behind and forwards.\n",
    "\n",
    "That gives us a bidirectional context vector for each word.\n",
    "\n",
    "The Pytorch RNN function already has bidrecitonal capability. It return the encoding in both directions, and we just simple concat the values together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b4ef60f7-e316-493d-a1e7-abe81a29f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor([ [0,1,2,3] ]).long() # Batch x Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e3f46ee0-86e3-4240-af01-5b3472ef82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "encoder = nn.GRU(embed_size, hidden_size, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "543307f3-116f-436d-b04d-b4593f7e34b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 620])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = encoder_embedding(token_ids)  # Batch x Sequence x Embedding Dimension\n",
    "embedding.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "74032bae-cc60-416a-9d76-64173861752b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2000])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out, hidden = encoder(embedding)\n",
    "encoder_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2bece7bf-4592-44b3-b9c5-bbb816258a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1000])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71869d1-6d80-4f4a-b5f4-04f69157505c",
   "metadata": {},
   "source": [
    "From https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "\n",
    "For bidirectional GRUs, forward and backward are directions 0 and 1 respectively.\n",
    "\n",
    "In the paper, the decoder initial hidden state uses the last hidden state from the encoder. Which is the first state in the backwards pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e40f72b8-d349-4da3-9e60-ac6bbb64bb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = hidden[1][0:]\n",
    "hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d4c0c-3f5a-48f3-a315-c05d85f64fef",
   "metadata": {},
   "source": [
    "Here I wrap all into a PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3d3afa38-e9b7-43bb-863d-1ac3196ad391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        hidden = hidden[1][0:]\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fa313d8f-40ed-4215-b022-48d560329a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 2000]), torch.Size([1, 1000]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(vocab_size, embed_size, hidden_size)\n",
    "encoder_outputs, encoder_hidden = enc(token_ids)\n",
    "encoder_outputs.shape, encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a99eb-4039-44ed-bca8-f3a4f959fd09",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d38bfb-e65b-434d-9609-3e7ffbc36a38",
   "metadata": {},
   "source": [
    "The initial hidden state $s_0$ is computed by $s_0 = \\tanh \\left( W_s \\overleftarrow{h}_1 \\right)$ where $W_s \\in \\mathbb{R}^{n \\times n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "14974219-85f6-4ea6-9a94-65659ead6c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1000])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_state = nn.Linear(hidden_size, hidden_size)\n",
    "dec_hidden = torch.tanh(init_state(encoder_hidden)).unsqueeze(0)\n",
    "dec_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5270406-bdae-4f0a-87e3-2ff19a1d565f",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239ca8a-97e0-4819-be23-d4e78e7e24bc",
   "metadata": {},
   "source": [
    "For the encoder outputs, we do a linearation project into the context size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5fb74d57-4eef-4c58-abd7-bf947f83a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_context = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "# As per the paper, we only use one direction of final hidden state. So doesn't need to be doubled.\n",
    "attention_hidden = nn.Linear(hidden_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "11965bc4-4c6b-426b-b3a1-ad450bb0bdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1000])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_proj = attention_context(outputs)\n",
    "context_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "086728b1-29e6-40f2-96ab-c0e290401861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1000])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_proj = attention_hidden(dec_hidden)\n",
    "hidden_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "43da00ec-ce7d-43d8-af5d-876394076cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1000])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_vector = torch.tanh(hidden_proj + context_proj)\n",
    "attention_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "555b8083-6733-4522-959a-616427daaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_alignment = nn.Linear(hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6f0d16b9-04a8-4bf9-aa7b-87660f67fb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = attention_alignment(attention_vector)\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4bf42bdb-00e2-4e5d-83c6-7ff15550521a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = attention_alignment(attention_vector).squeeze(2)\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0b03bff9-dc1e-4686-aee7-742c8534e82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1133, 0.1081, 0.0164, 0.0054]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d345370-6d20-45ee-b490-ce95b3c52647",
   "metadata": {},
   "source": [
    "Then a projection of the decoder hidden.\n",
    "\n",
    "Finally a Softmax is performed to convert into a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "eb11d7f4-111b-4776-9127-707100ce5732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2632, 0.2618, 0.2388, 0.2362]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = F.softmax(attention_scores, dim=1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "42a4936c-2074-4fbd-b3f5-49ebd1d461e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f795e-4860-44a9-ba39-d3e42ea3b96d",
   "metadata": {},
   "source": [
    "And now we do a matrix multiplication operation, which gives us the final weighed sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7a439b88-7c00-4751-a3ad-09dcfeb433fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2000])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.bmm(attention_weights.unsqueeze(1), outputs)\n",
    "context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7197b69-3814-41fd-b3ad-81d03b85eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_side):\n",
    "        self.attention_context = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.attention_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attention_alignment = nn.Linear(hidden_size, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4d4ff-a24e-43c7-9e1f-5bb36ecf82e4",
   "metadata": {},
   "source": [
    "### RNN Encoder-Decoder\n",
    "\n",
    "Start with the basic RNN Encoder-Decoder, called `RNNencdec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5420f7-0b88-4904-873c-05c39e084aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNencdec(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, embed_size, hidden_size)\n",
    "    \n",
    "         # Embedding layers\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.decoder = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Deep output layer with maxout\n",
    "        self.maxout = MaxoutLayer(hidden_size + embed_size, maxout_size)\n",
    "        self.final = nn.Linear(maxout_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=1.0):\n",
    "        batch_size = src.size(0)\n",
    "        max_len = tgt.size(1)\n",
    "        vocab_size = self.final.out_features\n",
    "        \n",
    "        # Initialize outputs tensor\n",
    "        outputs = torch.zeros(batch_size, max_len, vocab_size).to(src.device)\n",
    "        \n",
    "        # Encoder\n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_input = tgt[:, 0]\n",
    "        for t in range(1, max_len):\n",
    "            embedded_tgt = self.decoder_embedding(decoder_input)\n",
    "            decoder_output, hidden = self.decoder(embedded_tgt.unsqueeze(1), hidden)\n",
    "            \n",
    "            # Deep output\n",
    "            concat = torch.cat((decoder_output.squeeze(1), embedded_tgt), dim=1)\n",
    "            maxout_out = self.maxout(concat)\n",
    "            output = self.final(maxout_out)\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "            # Teacher forcing\n",
    "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
    "            decoder_input = tgt[:, t] if teacher_force else output.argmax(1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6edc1-c481-446e-8f3d-f44a4d032fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde6817-8480-45e6-8b3d-13ebe39dbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_enc = RNNencdec(vocab_size, embed_size, hidden_size)\n",
    "rnn_enc(token_ids, token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb904ee-c7e6-45c7-823a-ca1bf7bdf64e",
   "metadata": {},
   "source": [
    "#### Model Size\n",
    "\n",
    "We train two types of models. The first one is an RNN Encoder–Decoder (RNNencdec, Cho et al.,\n",
    "2014a), and the other is the proposed model, to which we refer as RNNsearch. We train each model\n",
    "twice: first with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then\n",
    "with the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50).\n",
    "\n",
    "\n",
    "The encoder and decoder of the RNNencdec have 1000 hidden units each.7 The encoder of the\n",
    "RNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000\n",
    "hidden units. Its decoder has 1000 hidden units. In both cases, we use a multilayer network with a\n",
    "single maxout (Goodfellow et al., 2013) hidden layer to compute the conditional probability of each\n",
    "target word (Pascanu et al., 2014).\n",
    "\n",
    "We use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler,\n",
    "2012) to train each model. Each SGD update direction is computed using a minibatch of 80 sentences. We trained each model for approximately 5 days.\n",
    "Once a model is trained, we use a beam search to find a translation that approximately maximizes the\n",
    "conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al., 2013). Sutskever\n",
    "et al. (2014) used this approach to generate translations from their neural machine translation model.\n",
    "For more details on the architectures of the models and training procedure used in the experiments,\n",
    "see Appendices A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb9b98-382a-4795-932f-38546418fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "rnn = nn.GRU(\n",
    "    embed_dim,\n",
    "    hidden_dim,\n",
    "    batch_first=True,\n",
    "    bidirectional=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2dd96-3e0a-48f1-879f-aa8027c716bb",
   "metadata": {},
   "source": [
    "Firstly we embedding the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986fef8-82ff-446f-8fb8-5a7ee5b4690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokens[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe0216-b03c-4e6c-9a69-0e514b95b652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa6bb1-5a20-4bb7-a3d6-c76ec2ae9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "embed_dim = 256\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "embeddings = embedding(en_tokens[\"input_ids\"])\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5d9739-bf3b-4ec4-b1d7-533d6a12977d",
   "metadata": {},
   "source": [
    "Now we can pass the sequence of embedding vectors into the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d05b91-fd86-4da2-8329-01403f92fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, hidden = rnn(embeddings)\n",
    "outputs.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6afd3b-df6f-4ba2-b87f-388d0732fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_outputs = torch.concat([outputs[0], outputs[-1]], dim=1)\n",
    "bidirectional_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664a685-d46e-496f-904f-de29f5035f53",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc6ae1-0ec2-4e0a-965e-b7b05909e071",
   "metadata": {},
   "source": [
    "The decoder first embeds the sequence. At inference time, we will provide a sequence with a character that represent starts of sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a17e7dcd-91cd-40c6-ae3f-06d1da249b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1]), tensor([0]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.tensor([0]).long()\n",
    "output.shape, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d5c0e-5b01-403c-8385-a68403526bc2",
   "metadata": {},
   "source": [
    "Let's start by building the components of Attention from scratch and visual what each is doing.\n",
    "\n",
    "Firstly, we create a `attention_hidden` Linear layer, a `attention_context` Linear layer and an `attention_alignment` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "483af354-2c0f-4ec0-ad89-3fc043214dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
    "attention_context = nn.Linear(hidden_size * 2, hidden_size)\n",
    "attention_alignment = nn.Linear(hidden_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da416f6a-8ec0-471c-8e77-040e10e5fd95",
   "metadata": {},
   "source": [
    "Now, we create a \"hidden project' using the decoder state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "501d991d-9342-431c-bf30-07878355b273",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hidden_projection \u001b[38;5;241m=\u001b[39m attention_hidden(\u001b[43mdecoder_state\u001b[49m)\n\u001b[1;32m      2\u001b[0m hidden_projection\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_state' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_projection = attention_hidden(decoder_state)\n",
    "hidden_projection.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8030b185-48fe-44b8-9bc3-3ad7736218b2",
   "metadata": {},
   "source": [
    "Now a projection for the \"context\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e6a64e-cf50-4c72-aa36-e34e975fe1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_projection = attention_context(encoder_outputs)\n",
    "context_projection.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c373499c-3b60-4a5d-80de-fb42a2d76730",
   "metadata": {},
   "source": [
    "Now we apply a `tanh` layer. Need to find this in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a428110-ddb2-4905-a566-b104f7bb133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_vector = torch.tanh(hidden_projection + context_projection)\n",
    "attention_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c8068-aac8-4919-b23b-56d3317d9d5d",
   "metadata": {},
   "source": [
    "Next we calculate the scores using the Attention Alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b4231-b5c5-4198-b0de-fd28ae46a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = attention_alignment(attention_vector).squeeze(2)\n",
    "attention_scores.shape\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a1d5f-48fb-4907-940e-c0a1d236dd73",
   "metadata": {},
   "source": [
    "Apply the attention mask to mask out future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57563f15-cefa-4a8d-bcd0-20f415ce9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = attention_scores.masked_fill(\n",
    "    ~attention_mask.bool(), \n",
    "    float('-inf')\n",
    ")\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641159b-3732-40a5-b206-f1e67a17d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = F.softmax(attention_scores, dim=1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec1cc8-92b4-4acc-ab9a-2bdd3eb4fc6a",
   "metadata": {},
   "source": [
    "Now a \"context\" item. Not sure what this is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ac29e-5d9c-464e-be41-8a9911092b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "context.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082f501-694b-4b9f-ae61-3d0b9a83d4dd",
   "metadata": {},
   "source": [
    "Next up, let me review that in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b3db3-dabf-4b64-98ef-4be2abd9c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.attention_context = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.attention_alignment = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, decoder_state, encoder_outputs, attention_mask):\n",
    "        hidden_projection = self.attention_hidden(decoder_state)\n",
    "        context_projection = self.attention_context(encoder_outputs)\n",
    "        \n",
    "        attention_vector = torch.tanh(hidden_projection + context_projection)\n",
    "        attention_scores = self.attention_alignment(attention_vector).squeeze(2)\n",
    "        \n",
    "        # Apply attention mask\n",
    "        attention_scores = attention_scores.masked_fill(\n",
    "            ~attention_mask.bool(), \n",
    "            float('-inf')\n",
    "        )\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        \n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f4a81-745d-45c1-80f8-b5c49f371b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_batch(batch_size=32, max_src_len=20, max_tgt_len=20, \n",
    "                       src_vocab_size=1000, tgt_vocab_size=1000):\n",
    "    src = torch.randint(0, src_vocab_size, (batch_size, max_src_len))\n",
    "    tgt = torch.randint(0, tgt_vocab_size, (batch_size, max_tgt_len))\n",
    "    src_lengths = torch.randint(5, max_src_len + 1, (batch_size,))\n",
    "    return src, tgt, src_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf58c65-0760-4814-94d2-eb889e85fa0b",
   "metadata": {},
   "source": [
    "Let's create some sample batches firstly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31702fef-5fbd-4881-9dd3-56c776ef1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "max_src_len=20\n",
    "max_tgt_len=20\n",
    "src_vocab_size=1000\n",
    "tgt_vocab_size=1000\n",
    "\n",
    "source = torch.randint(0, src_vocab_size, (batch_size, max_src_len))\n",
    "target = torch.randint(0, tgt_vocab_size, (batch_size, max_tgt_len))\n",
    "source_lengths = torch.randint(5, max_src_len + 1, (batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f5921-4f62-4236-8a6f-6e9097c27e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5147d4-cc8c-45f5-822e-71129b1d542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32f7a1-dfd1-4963-86bf-277152d0800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8608d6-af50-4ee7-91da-89f7b3a5c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, src, src_lengths):\n",
    "        embedded = self.embedding(src)\n",
    "\n",
    "        outputs, hidden = self.rnn(packed)\n",
    "        \n",
    "        # Combine bidirectional outputs\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98decd37-e8b3-4885-9f96-0c10d69c20e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, attention, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.attention = attention\n",
    "        \n",
    "        # Input size is embedding + context vector\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_dim + hidden_dim * 2,\n",
    "            hidden_dim * 2,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, attention_mask):\n",
    "        # input shape: [batch_size, 1]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # Get context vector from attention\n",
    "        context, attention_weights = self.attention(\n",
    "            hidden.unsqueeze(1), encoder_outputs, attention_mask\n",
    "        )\n",
    "        \n",
    "        # Combine embedding and context vector\n",
    "        rnn_input = torch.cat([embedded, context], dim=2)\n",
    "        \n",
    "        # Pass through RNN\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        # Generate output distribution\n",
    "        prediction = self.output(output.squeeze(1))\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d2327-8822-400c-8adb-6f59daec406d",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "For each sentence, we tokenise by splitting on space. Later, I'll find the actual tokeniser they used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb542f7c-b4aa-4946-9007-2b24c81fe195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f4b48-5043-433d-8c7a-f041ef9b17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenise = input_sentence.split()\n",
    "tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c7673-c87c-4eef-a1ec-1b2c319abd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(tokenise)))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff07c1-de54-4022-8f20-c222a53d1e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f3072-490d-43a1-b74e-2b78a3a63b52",
   "metadata": {},
   "source": [
    "Now we can numerise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8becc12-34c6-4af8-8307-59e314b35222",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [vocab.index(token) for token in tokenise]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13eeb55-7ebd-4558-b8a1-7f46046f2f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a44705-3cd4-4c27-9249-31f0c21c29cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1204eb-5833-4f71-b54d-4e3f1eb13efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434126e-f042-4843-8695-216f860fac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ac7a1-05c8-4846-8683-8877019cf8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(\n",
    "    embed_dim,\n",
    "    hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    batch_first=True,\n",
    "    bidirectional=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be1faee-3f90-4f63-a8e8-5ff040c78700",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, hidden = rnn(embeddings)\n",
    "outputs.shape, hidden[0,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b64397-2119-45b3-a2a8-dac204ac4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dropout = nn.Dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5faaa-551e-437f-905e-d1ff00b9264d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a4047-e64a-4f8c-8326-7b740a5f6da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e91d5-1e34-4715-a83c-2c789e4874c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a646c19-ca44-4d57-9aea-cd6c5ac1d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = pack_padded_sequence(\n",
    "    embedded, torch.tensor([1]).cpu(), batch_first=True, enforce_sorted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736bc186-69c6-4b2e-9d99-74ff049a44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc549ff7-a682-4d7a-8869-77589fdb9384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ed3fb-055d-43c3-9e93-78ebcb3db25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, hidden = rnn(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb0b60-30fd-497a-8883-cf1114bd1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the sequence\n",
    "outputs, _ = pad_packed_sequence(outputs, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165daf13-9dc3-4ea7-b471-e8ddac5dbb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine bidirectional outputs\n",
    "hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48d0c0-1afd-4795-8f5d-5dbb949f3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e80630-acea-4ead-bcc7-f01e7fb1d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e463fa94-459f-4c3c-8439-efe4d3ee4bce",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dfe085-afba-4704-9d63-0f6d8d376212",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce678381-9d7f-4678-8b1f-68255b1546d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sentence = \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df7f21-7ecb-488f-a955-bb3a1aa85999",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embedding = nn.Embedding(vocab_size, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580179c-7b79-4ec6-b810-60d188f0e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Input size is embedding + context vector\n",
    "decoder_rnn = nn.GRU(\n",
    "    embed_dim + hidden_dim * 2,\n",
    "    hidden_dim * 2,\n",
    "    num_layers=num_layers,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "dropout=0.1\n",
    "decoder_output = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "decoder_dropout = nn.Dropout(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90236fd3-49a1-4f76-be02-6c1a92d3a5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
