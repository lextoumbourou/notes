{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## RNNSearch - \"Neural Machine Translation by Jointly Learning to Align and Translate\"\n\nThis notebook is a quick implementation of `RNNSearch` from paper [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473).","metadata":{}},{"cell_type":"code","source":"from typing import Tuple\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom transformers import AutoTokenizer\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom tokenizers.models import WordPiece\nfrom transformers import BertTokenizerFast\n\nimport datasets","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:06:04.643254Z","iopub.execute_input":"2024-10-30T03:06:04.643622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Configuration","metadata":{}},{"cell_type":"markdown","source":"For all the models used in this paper:\n* the size of a hidden layer $n$ is 1000\n* the word embedding dimensionality $m$ is 620\n* size of the maxout hidden layer in the deep output $l$ is 500.\n* The number of hidden units in the alignment model $n$ is 1000.\n\nI've had to adjust to fit in the GPU limits.","metadata":{}},{"cell_type":"code","source":"#embed_size = 620\n#hidden_size = 1000\n#maxout_size = 500\n\nembed_size = 128\nhidden_size = 128\nmaxout_size = 128\nvocab_size = 32000\nmax_length = 10\nbatch_size = 4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoder","metadata":{}},{"cell_type":"code","source":"token_ids = torch.tensor([ [0,1,2,3] ]).long() # Batch x Sequence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_embedding = nn.Embedding(vocab_size, embed_size)\nencoder = nn.GRU(embed_size, hidden_size, batch_first=True, bidirectional=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding = encoder_embedding(token_ids)  # Batch x Sequence x Embedding Dimension\nembedding.shape ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_out, hidden = encoder(embedding)\nencoder_out.shape, hidden.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.rnn = nn.GRU(\n            embed_dim,\n            hidden_dim,\n            batch_first=True,\n            bidirectional=True\n        )\n        \n    def forward(self, src):\n        embedded = self.embedding(src)\n\n        outputs, hidden = self.rnn(embedded)\n        hidden = hidden[1][0:]\n\n        return outputs, hidden","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enc = Encoder(vocab_size, embed_size, hidden_size)\nencoder_outputs, encoder_hidden = enc(token_ids)\nencoder_outputs.shape, encoder_hidden.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attention Mechanism","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, encoder_hidden_size, decoder_hidden_size, alignment_hidden_size):\n        super(Attention, self).__init__()\n\n        self.decoder_hidden_layer = nn.Linear(decoder_hidden_size, alignment_hidden_size)\n        self.encoder_outputs_layer = nn.Linear(encoder_hidden_size, alignment_hidden_size)\n        self.score_layer = nn.Linear(alignment_hidden_size, 1)\n\n    def forward(self, decoder_hidden_state, encoder_outputs):\n        projected_decoder_state = self.decoder_hidden_layer(decoder_hidden_state.squeeze(0))\n        projected_decoder_state = projected_decoder_state.unsqueeze(1)  # [batch, 1, hidden]\n\n        projected_encoder_outputs = self.encoder_outputs_layer(encoder_outputs)\n\n        alignment_scores = torch.tanh(projected_decoder_state + projected_encoder_outputs)\n        alignment_scores = self.score_layer(alignment_scores).squeeze(2)  # (batch_size, sequence_length)\n\n        # Apply softmax to get alignment weights\n        alignment_weights = F.softmax(alignment_scores, dim=1)\n        \n        alignment_weights_expanded = alignment_weights.unsqueeze(2)\n    \n        context_vector = torch.sum(encoder_outputs * alignment_weights_expanded, dim=1)\n\n        return context_vector, alignment_weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dec_hidden = torch.randn(80, hidden_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attn = Attention(hidden_size * 2, hidden_size, hidden_size)\ncontext_vector, alignment_weights = attn(dec_hidden, encoder_out)\ncontext_vector.shape, alignment_weights.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Maxout Layer","metadata":{}},{"cell_type":"markdown","source":"The final layer of the decoder is a Maxout layer, which projects a linear layer into two buckets and takes the max. A form of regularisation.","metadata":{}},{"cell_type":"code","source":"class MaxoutLayer(nn.Module):\n    def __init__(self, input_size, output_size, num_pieces=2):\n        super().__init__()\n        self.linear = nn.Linear(input_size, output_size * num_pieces)\n        self.num_pieces = num_pieces\n\n    def forward(self, x):\n        output = self.linear(x)\n        output = output.view(-1, self.num_pieces, output.size(1) // self.num_pieces)\n        output, _ = torch.max(output, dim=1)\n\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxout_layer = MaxoutLayer(\n    input_size=hidden_size * 3 + embed_size,\n    output_size=vocab_size,\n    num_pieces=2\n)\n\nhidden_states = torch.randn(1, hidden_size * 3 + embed_size)\nmaxout_layer(hidden_states).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decoder","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, dropout: float = 0.1):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        \n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        \n        self.attention = Attention(hidden_size * 2, hidden_size, hidden_size)\n        \n        self.gru = nn.GRU(hidden_size * 2 + embed_size, hidden_size, batch_first=True)\n        \n        self.maxout = MaxoutLayer(\n            input_size=hidden_size + hidden_size * 2 + embed_size, \n            output_size=vocab_size,\n            num_pieces=2\n        )\n\n    def forward(self, input: torch.Tensor, hidden: torch.Tensor, encoder_outputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        embedded = self.embedding(input)\n        \n        context, attn_weights = self.attention(hidden, encoder_outputs)\n        \n        rnn_input = torch.cat((embedded, context), dim=1).unsqueeze(1)  # [batch_size, 1, hidden_size * 2 + embed_size]\n    \n        output, hidden = self.gru(input=rnn_input, hx=hidden)\n        \n        maxout_input = torch.cat((hidden[0], context, embedded), dim=1)\n        \n        # Note that Softmax will be applied in loss calculation.\n        prediction_scores = self.maxout(maxout_input)  # [batch_size, output_dim]\n        \n        return prediction_scores, hidden, attn_weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dec = Decoder(vocab_size, embed_size, hidden_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 80\n\ninput_tensor = torch.tensor([[0, 1, 2, 3]] * 80).long()\nhidden_tensor = torch.randn(1, batch_size, hidden_size)\nencoder_outputs = torch.randn(batch_size, 4, hidden_size * 2) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction, hidden, attn_weights = dec(input=input_tensor[:,0], hidden=hidden_tensor, encoder_outputs=encoder_outputs)\nprediction.shape, hidden.shape, attn_weights.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Implementation","metadata":{}},{"cell_type":"code","source":"class RNNSearch(nn.Module):\n    def __init__(self, vocab_size, hidden_size, output_size, sos_token):\n        super(RNNSearch, self).__init__()\n\n        self.encoder = Encoder(vocab_size, embed_size, hidden_size)\n        self.decoder = Decoder(vocab_size, embed_size, hidden_size)\n        \n        self.decoder_init = nn.Linear(hidden_size, hidden_size)\n    \n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.sos_token = sos_token\n        \n    def init_decoder(self, encoder_hidden):\n        return torch.tanh(self.decoder_init(encoder_hidden)).unsqueeze(0)\n\n    def forward(self, input, target, target_length):\n        batch_size = input.shape[0]\n        \n        # Encoding\n        encoder_outputs, encoder_hidden = self.encoder(input)\n\n        # Initialise hidden state.\n        decoder_hidden = self.init_decoder(encoder_hidden)\n        \n        # Initial input is SOS token.\n        decoder_input = torch.tensor([self.sos_token] * batch_size).to(input.device)\n\n        outputs = []\n        for i in range(target.shape[-1]):\n            decoder_output, decoder_hidden, _ = self.decoder(\n                input=decoder_input,\n                hidden=decoder_hidden,\n                encoder_outputs=encoder_outputs\n            )\n            outputs.append(decoder_output)\n            \n            # Teacher forcing: next input is current target\n            decoder_input = target[:, i]\n\n        return torch.stack(outputs, dim=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create random input data\nsource = torch.randint(0, vocab_size, (batch_size, 80))  # Random source sentences\ntarget = torch.randint(0, vocab_size, (batch_size, 80))  # Random target sentences\n\n# Initialize model\nmodel = RNNSearch(\n    vocab_size=vocab_size,\n    hidden_size=hidden_size,\n    output_size=vocab_size,\n    sos_token=vocab_size-1\n)\n\n# Test forward pass\noutput = model(\n    input=source,\n    target=target,\n    target_length=10\n)\n\nmodel = None\n\n# Print shapes\nprint(\"Input shape:\", source.shape)\nprint(\"Target shape:\", target.shape)\nprint(\"Output shape:\", output.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"markdown","source":"The paper demonstrates the approach on an English to French translation task, using the data provided as part of the [Workshop on Statistical Machine Translation in 2014](https://aclanthology.org/W14-3302.pdf). I've found a version of that on HuggingFace. Not sure exactly how closely it mirrors the paper, but I'm not too concerned.","metadata":{}},{"cell_type":"code","source":"dataset = datasets.load_dataset(\"presencesw/wmt14_fr_en\")\nprint(\"Dataset structure:\", dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the paper, they \"concat news-test-2012 and news-test-2013\" for the validation set, but I'm using the validation set kindly provided by presencesw.","metadata":{}},{"cell_type":"markdown","source":"## Tokeniser\n\nThe paper uses the Moses tokeniser, however, I'm going to use a multi-lingual tokeniser from HuggingFace, as it comes with a few features that makes life easier.","metadata":{}},{"cell_type":"code","source":"example = dataset['train'][0]\nexample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    \"facebook/mbart-large-cc25\", model_max_length=max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([tokenizer.decode(t) for t in tokenizer(example[\"en\"])[\"input_ids\"]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([tokenizer.decode(t) for t in tokenizer(example[\"fr\"])[\"input_ids\"]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Dataset and Dataloader\n\n","metadata":{}},{"cell_type":"code","source":"# Create dataset and dataloader\nclass TranslationDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        src_tokens = self.tokenizer(item['en'], \n                                  padding='max_length',\n                                  truncation=True,\n                                  max_length=self.max_len,\n                                  return_tensors='pt')\n        tgt_tokens = self.tokenizer(item['fr'],\n                                  padding='max_length',\n                                  truncation=True,\n                                  max_length=self.max_len,\n                                  return_tensors='pt')\n        \n        return {\n            'src': src_tokens['input_ids'].squeeze(),\n            'tgt': tgt_tokens['input_ids'].squeeze(),\n            'tgt_len': len(tgt_tokens['input_ids'][0])\n        }\n\n# Create dataloaders\ntrain_dataset = TranslationDataset(dataset['train'].select(range(10000)), tokenizer, max_len=max_length)\nval_dataset = TranslationDataset(dataset['validation'], tokenizer, max_len=max_length)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Training setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = RNNSearch(\n    vocab_size=len(tokenizer),\n    hidden_size=hidden_size,\n    output_size=len(tokenizer),\n    sos_token=tokenizer.bos_token_id\n).to(device)\n\n# Adadelta optimizer as used in the paper\noptimizer = torch.optim.Adadelta(model.parameters(), lr=1.0, rho=0.95, eps=1e-6)\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\ndef validate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Validating\"):\n            src = batch['src'].to(device)\n            tgt = batch['tgt'].to(device)\n            tgt_len = batch['tgt_len']\n            \n            output = model(src, tgt, tgt_len)\n            output = output.view(-1, model.output_size)\n            target = tgt.view(-1)\n            \n            loss = criterion(output, target)\n            total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\ndef train_epoch(model, dataloader, optimizer, criterion, device, clip_value=5.0):\n    model.train()\n    total_loss = 0\n    \n    for batch in tqdm(dataloader, desc=\"Training\"):\n        src = batch['src'].to(device)\n        tgt = batch['tgt'].to(device)\n        tgt_len = batch['tgt_len']\n        \n        optimizer.zero_grad()\n        \n        output = model(src, tgt, tgt_len)\n        output = output.view(-1, model.output_size)\n        target = tgt.view(-1)\n        \n        loss = criterion(output, target)\n        loss.backward()\n        \n        # Clip gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n        \n        optimizer.step()\n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\n# Training loop\nbest_valid_loss = float('inf')\nn_epochs = 100\npatience = 5\nno_improvement = 0\n\nprint(f\"Training on {device}\")\n\nfor epoch in range(n_epochs):\n    train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device)\n    valid_loss = validate(model, val_dataloader, criterion, device)\n    \n    print(f'Epoch: {epoch+1:02}')\n    print(f'\\tTrain Loss: {train_loss:.3f}')\n    print(f'\\tValid Loss: {valid_loss:.3f}')\n    \n    # Save best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'best-model.pt')\n        no_improvement = 0\n    else:\n        no_improvement += 1\n    \n    # Early stopping\n    if no_improvement >= patience:\n        print(\"Early stopping triggered\")\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}