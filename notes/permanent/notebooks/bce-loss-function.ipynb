{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In code, we can express the BCE loss function as follows:","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np\nfrom torch import tensor, where, nn\n\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\ndef binary_cross_entropy_single_label(logit, label):\n    pred = sigmoid(logit)\n\n    if label == 1:\n        return -math.log(pred)\n    \n    return -math.log(1-pred)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-08T06:58:47.614883Z","iopub.execute_input":"2021-08-08T06:58:47.615358Z","iopub.status.idle":"2021-08-08T06:58:48.074790Z","shell.execute_reply.started":"2021-08-08T06:58:47.615243Z","shell.execute_reply":"2021-08-08T06:58:48.073836Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"logit = 5\nlabel = 1\n\nbinary_cross_entropy_single_label(logit, label)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T06:58:48.076616Z","iopub.execute_input":"2021-08-08T06:58:48.076914Z","iopub.status.idle":"2021-08-08T06:58:48.085341Z","shell.execute_reply.started":"2021-08-08T06:58:48.076887Z","shell.execute_reply":"2021-08-08T06:58:48.084202Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"0.006715348489117944"},"metadata":{}}]},{"cell_type":"markdown","source":"We can express the same function in math as follows:\n\n$$p = \\text{sigmoid}(o)$$\n$$L(p, y) = ‚àí(ùë¶ \\times log(ùëù) + (1‚àíùë¶) \\times log(1‚àíùëù))$$","metadata":{}},{"cell_type":"markdown","source":"For multi-label outputs, the function takes the mean (or some other reduction method) each of the log loss values:","metadata":{}},{"cell_type":"code","source":"def binary_cross_entropy(logits, labels):\n    return np.mean([\n        binary_cross_entropy_single_label(logit, label) for logit, label in zip(logits, labels)])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T06:58:48.087053Z","iopub.execute_input":"2021-08-08T06:58:48.087357Z","iopub.status.idle":"2021-08-08T06:58:48.097463Z","shell.execute_reply.started":"2021-08-08T06:58:48.087327Z","shell.execute_reply":"2021-08-08T06:58:48.096355Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"logits = [5, -2, 0.5]\nlabels = [1, 0, 1]\n\nbinary_cross_entropy(logits, labels)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T06:58:48.099152Z","iopub.execute_input":"2021-08-08T06:58:48.099522Z","iopub.status.idle":"2021-08-08T06:58:48.113780Z","shell.execute_reply.started":"2021-08-08T06:58:48.099483Z","shell.execute_reply":"2021-08-08T06:58:48.112551Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0.2025734479040657"},"metadata":{}}]},{"cell_type":"markdown","source":"Which we express in math as follows:\n\n$$P = \\text{sigmoid}(O)$$\n$$L(P, Y) = ‚àí\\frac{1}{N} \\sum\\limits_{i=1}^{N} (Y_{i} \\times log(P_{i}) + (1‚àí Y_{i}) \\times log(1‚àí P_{i}))$$","metadata":{}},{"cell_type":"markdown","source":"PyTorch provides the function via the [`nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) class, which expects you first to apply the Sigmoid function to the model's outputs. It's the equivalent of [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) in multi-class classification.\n\nUse [`nn.BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) if your model doesn't perform the Sigmoid Activation Function on the final layer.","metadata":{}},{"cell_type":"code","source":"logits = tensor([5, -2, 0.5]).float()\nlabels = tensor([1, 0, 1]).float()\n\nnn.BCEWithLogitsLoss()(logits, labels)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T06:58:48.114899Z","iopub.execute_input":"2021-08-08T06:58:48.115187Z","iopub.status.idle":"2021-08-08T06:58:48.131941Z","shell.execute_reply.started":"2021-08-08T06:58:48.115160Z","shell.execute_reply":"2021-08-08T06:58:48.130684Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"tensor(0.2026)"},"metadata":{}}]},{"cell_type":"markdown","source":"Which is the equivalent of the following function:","metadata":{}},{"cell_type":"code","source":"def binary_cross_entropy(logits, labels):\n    preds = logits.sigmoid()\n    return -where(labels==1, preds, 1-preds).log().mean()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T06:58:54.639242Z","iopub.execute_input":"2021-08-08T06:58:54.639677Z","iopub.status.idle":"2021-08-08T06:58:54.645031Z","shell.execute_reply.started":"2021-08-08T06:58:54.639640Z","shell.execute_reply":"2021-08-08T06:58:54.643925Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"binary_cross_entropy(logits, labels)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T06:59:00.388992Z","iopub.execute_input":"2021-08-08T06:59:00.389476Z","iopub.status.idle":"2021-08-08T06:59:00.400706Z","shell.execute_reply.started":"2021-08-08T06:59:00.389437Z","shell.execute_reply":"2021-08-08T06:59:00.399196Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"tensor(0.2026)"},"metadata":{}}]}]}