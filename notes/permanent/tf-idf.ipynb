{
 "cells": [
  {
   "cell_type": "raw",
   "id": "31a5f96b",
   "metadata": {},
   "source": [
    "---\n",
    "title: TF-IDF\n",
    "date: 2023-04-09 00:00\n",
    "modified: 2025-12-20 00:00\n",
    "summary: A word vectorisation technique that weights terms by their importance to a document relative to a corpus.\n",
    "cover: /_media/tf-idf-cover.png\n",
    "tags:\n",
    "- NaturalLanguageProcessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0e52a",
   "metadata": {},
   "source": [
    "**TF-IDF** (Term Frequency - Inverse Document Frequency) is a numerical statistic used in Natural Language Processing to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "The fundamental intuition is that:\n",
    "\n",
    "1. Words that appear frequently in a specific document should be weighted higher.\n",
    "2. Words that appear frequently across all documents (like \"the\", \"is\", and \"and\") have less signal and are weighted lower.\n",
    "\n",
    "#### How it works:\n",
    "\n",
    "**1. Term Frequency (TF)**\n",
    "\n",
    "Measures how frequently a term occurs in a document.\n",
    "\n",
    "$$\\text{tf}(t, d) = \\frac{\\text{count of \\textit{t} in \\textit{d}}}{\\text{total words in \\textit{d}}}$$\n",
    "\n",
    "**2. Inverse Document Frequency (IDF)**\n",
    "\n",
    "$$\\text{idf}(t) = \\log\\left(\\frac{N}{1 + df_t}\\right)$$\n",
    "\n",
    "Measures how rare a term is across the entire corpus of documents.\n",
    "\n",
    "* $N$: Total number of documents.\n",
    "* $df_t$: Number of documents containing the term.\n",
    "* The `+1` (smoothing) prevents division by zero if a term isn't in the training corpus.\n",
    "* The `log` function dampens the magnitude of the IDF weight, ensuring that extremely rare words don't overpower the entire vector.\n",
    "\n",
    "**3. TF-IDF Score**\n",
    "\n",
    "The final score is the product of these two metrics:\n",
    "\n",
    "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n",
    "\n",
    "Words that are frequent in a specific document but rare across the corpus receive the highest scores, making them the \"signature\" terms for that document.\n",
    "\n",
    "---\n",
    "\n",
    "We can visualise TF-IDF using a quick heatmap of 4 short documents with a limited vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs are great pets\",\n",
    "    \"the mat is on the floor\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "df_tfidf = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(), \n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(corpus))]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_tfidf, annot=True, cmap=\"YlGnBu\")\n",
    "plt.title(\"TF-IDF Scores Heatmap\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
