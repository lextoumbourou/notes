---
title: ReLU
date: 2023-12-10 00:00
modified: 2023-12-10 00:00
status: draft
tags:
  - Rectified linear unit
---

A simple nonlinear [Activation Function](activation-function.md) used in neural networks, which literally just sets negative values to 0: `max(activations, 0)`

