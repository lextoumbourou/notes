---
title: ReLU
date: 2024-01-09 00:00
modified: 2024-01-09 00:00
status: draft
tags:
  - Rectified linear unit
summary: A simple approach to non-linearity in a neural network.
---

A common and simple activation function used in a range of deep learning architectures.

The simplest conceivable way to add non-linearity to a neural network. And it works!

```
x = max(x, 0)
```
