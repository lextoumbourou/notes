---
title: Self-Attention
date: 2024-01-30 00:00
modified: 2024-01-30 00:00
status: draft
---

Self-Attention, sometimes called intra-attention, is an [Attention Mechanism](attention-mechanism.md) technique concerned with representing an input sequence as a weighted-average of the other token representations in the sequence, based on how important tokens are to each other.

The common implementation of self-attention comes from the [Transformer](transformer.md) architecture: [Scaled-Dot Product Attention](scaled-dot-product-attention.md)
