---
title: Arcface
date: 2022-04-23 00:00
category: reference/papers
summary: Notes from paper [ArcFace: Additive Angular Margin Loss for Deep Face Recognition](https://arxiv.org/pdf/1801.07698.pdf) by Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou
---

Notes from paper [ArcFace: Additive Angular Margin Loss for Deep Face Recognition](https://arxiv.org/pdf/1801.07698.pdf) by Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou

## Abstract

The main challenge for large-scale face recognition is designing a loss function with strong discriminative power.

[Centre loss](https://arxiv.org/abs/1707.07391) penalizes distance between features and corresponding class centers to achieve "compactness" within the class.

[SphereFace](https://arxiv.org/abs/1704.08063) lets the linear transformation matrix in the last fully-connected layer represent class-center and penalizes the angle between deep features and their corresponding weights.

A recently popular idea is incorporating "margins" to maximize face class separability.

This paper proposes Additive Angular Margin Loss (ArcFace), which creates "highly discriminative features" for face recognition.

ArcFace has a "clear geometric interpretation" due to its correspondence to "geodesic distance" (a curve representing the shortest path between 2 points) on the hypersphere.

This paper studies results of ArcFace on ten face recognition benchmarks and shows that ArcFace continually outperforms other algorithms. 

The authors released all training code and metadata.

## Intro

The method of choice right now for [[Face Recognition]] is to represent faces using an [[Embedding]] generated by a [[Convolutional Neural Network]].

The idea is the embedding has a small distance to other examples in the same class (intra-class) and a large distance to other classes (inter-class).

There's typically a pose normalization step (aligning faces) before generating embeddings.

There are two main tracks of research for training these embeddings:
  
1. Train a multi-class classifier and use the embedding generated by the network as the representation.
2. Directly learn an embedding, for example [[Triplet Loss]].

Both have their drawbacks.

For softmax loss, the linear transformation matrix $W$ size increases linearly with every identity `n`.

The learned features are separable enough for closed-set classification but not for open-set problems.

For triplet loss, there's a combinatorial explosion in the number of face triplets for large-scale datasets.

Semi-hard sample mining is quite a complex problem to engineer.

Others have proposed a number of variants of Softmax loss to enhance the discriminative power of Softmax loss.

Wen et al. l. pioneered center loss. It captures the distance between each feature vector and class center and requires joint penalization of softmax loss for intra-class dispersion. But it is challenging to update class centers when there are a lot of classes.

Instead, enforcing intra-class closes and inter-class separateness at every step should lead to better models.

That's the idea behind Sphereface, which introduced an angular margin. However, the loss function needs a series of approximations to be computed, resulting in unstable training. They fix this by proposing a hybrid loss function that includes softmax loss.

CosFace adds cosine margin penalty to the target logits:

`logits` + `some margin penalty`

This method provides better performance and is easier to train.

ArcFace is just a slight alternative to CosFace using arc-cosine instead of cosine.

---

In this paper, they propose Additive Angular Margin Loss (ArcFace) to improve the discriminative power of CosFace.

Since we know the dot product between the CNN feature and the last fully connected layer is equal to cosine distance after feature and weight normalization.

![Arcface Figure 2](/_media/arcface-figure-2.png)

They utilize arc-cosine function to calculate the angle between the current feature and target weight.

Then add additive angular margin to the target angle, and get the target logit back by a cosine function.

Rescale all logits by a fixed feature norm, and subsequent steps are the same as Softmax loss.

Its:

* Engaging - directly optimizes the geodesic distance margin by virtue of exact correspondence between angle and arc in normalised hypersphere.
* Effective - achieves state-of-the-art performance on ten face rec datasets.
* Easy - Only needs several lines of code to implement.
* Efficient - negligible computational overhead.

## Proposed Approach

### ArcFace