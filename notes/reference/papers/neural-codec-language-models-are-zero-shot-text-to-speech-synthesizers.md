---
title: "Neural Codec Language Models are Zero-Shot Text-to-Speech Synthesizers"
date: 2023-12-21 00:00
modified: 2023-12-21 00:00
status: draft
category: reference/papers
---

Notes from the paper [Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](https://arxiv.org/abs/2301.02111) by Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei

---

## Overview

 This paper describes a state-of-the-art [Zero-Shot TTS](../../permanent/zero-shot-tts.md) model called [VALL-E](../../permanent/vall-e.md). VALL-E receives text and a 3-second sample of a speaker and outputs new audio for that speaker, even if the speaker is not in training data. The author's juxtapose this capability with GPT-3's [In-Context Learning](../../permanent/in-context-learning.md) capability: the ability to learn information by providing additional context during inference.

The main breakthrough of this paper is utilising the **neural audio codec** [Encodec](../../../../permanent/encodec.md), which uses [Residual Vector Quantisation](../public/notes/permanent/residual-vector-quantisation.md) to converts audio into a discrete tokens. This tokenisation approach, allows them to model the problem using language models, subsequently allowing them to train on larger, more noisy datasets than previous TTS solutions and seriously improving the ability to generate unseen speakers.

Hence: <font color="blue">Neural Code</font> <font color="dark-yellow">Language Models</a> are <font color="orange">Zero-Shot</a> <font color="green">Text-to-Speech Synthesizes</a>.

![](../../../../_media/neural-codec-language-models-are-zero-shot-text-to-speech-synthesizers-fig-1.png)

The authors also introduce an interesting hierarchical approach to modelling codes, focusing different models on different codebook layers.

They do not release an open-source solution, though it is simple enough that many successful open-source reproductions exist.

### Key details

#### 1. Use RVQ instead of Mels for intermediary representation

The TTS approach of VALL-E is Cascading TTS. That means, the model's encodes text into an intermediate representation, before being decoded into audio.

Previous approaches have used a [Mel Spectrogram](../../permanent/melspectrogram.md) for intermediary relationship, and used a Vocoder to decode into audio.

VALL-E, like a lot of audio recent audio papers, uses the neural audio codec [Encodec](../../../../permanent/encodec.md) for intermediate representation, which tokenises audio into multiple streams of discrete codes, using [residual-vector-quantization](../../permanent/residual-vector-quantization.md).

#### 2. Train on 60k hours of speech from Libri-Light

Thanks to the discrete representation, the authors treat the problem as a language model, which allows them to scale up the dataset significantly. They train on 60k hours of audio from the [Libri-Light](https://github.com/facebookresearch/libri-light) dataset, hundreds of times more than existing TTS papers. Since most of the data is unannotated, they use an off-the-shelf ASR model to generate the annotations ([Hybrid Deep Neural Network--Hidden Markov Model](Hybrid%20Deep%20Neural%20Network--Hidden%20Markov%20Model))).

#### 3. Model the codes with a Hierarchical Language Model Architecture

Lastly, through careful study of the codes generated by Encodec, they reason that recovering audio from the first codes tends to include acoustic properties like speaker identity. In contrast, the later codes can reconstruct more fine acoustic details. Therefore, they take a hierarchical approach to modelling the codes and train two separate language models, one [Autoregressive](Autoregressive), which predicts the first code given the previous sequence, and another [Non-Autoregressive](Non-Autoregressive), which predicts the subsequent codes from the first codes.

### Introduction

Cascaded text-to-speech (TTS) systems usually involve:

1. **Acoustic Model** This converts text into an intermediate representation, commonly a [Mel Spectrogram](../public/notes/permanent/Mel spectrogram.md).
2. A **Vocoder** which converts the Mel into audio.

Papers that take this approach:

* [Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](../../../../permanent/natural-tts-synthesis-by-conditioning-wavenet-on-mel-spectrogram-predictions.md)
* [Fastspeech: Fast, robust and controllable text to speech](https://arxiv.org/abs/1905.09263)
* [Neural Speech Synthesis with Transformer Network](https://arxiv.org/abs/1809.08895)

End-to-end TTS approaches can synthesise high-quality speech from one or many speakers, however it requires high-quality and clean audio.

Papers that take this approach:

* [DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders](../../permanent/delightfultts-2-end-to-end-speech-synthesis-with-adversarial-vector-quantized-auto-encoders.md)
* [Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech](Conditional%20variational%20autoencoder%20with%20adversarial%20learning%20for%20end-to-end%20text-to-speech)

The downside to this approach is that data crawled from the internet generally does not improve speech quality. And these smaller datasets lead to models that don't generalise and don't work with zero-shot scenarios.

For the zero-shot TTS approach, some existing work has used "speaker adaptation":

* [Sample Efficient Adaptive Text-to-Speech](Sample%20Efficient%20Adaptive%20Text-to-Speech)
* [Spoken Content and Voice Factorization for Few-shot Speaker Adaptation](https://www.isca-speech.org/archive/pdfs/interspeech_2020/wang20g_interspeech.pdf)

Others have used speaker encoding:

* [Neural voice cloning with a few samples](https://arxiv.org/abs/1802.06006)
* [Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone](https://arxiv.org/abs/2112.02418)

But they all require additional fine-tuning, complex pre-designed features, or heavy structure engineering.

Training a model with a large and diverse dataset has worked so well for modelling text, but speech has not yet been able to take advantage of these types of datasets.

Related papers:
* [Language Models are Few-Shot Learners](../../permanent/language-models-are-few-shot-learners.md)
* [Palm: Scaling language modeling with pathways](Palm:%20Scaling%20language%20modeling%20with%20pathways)

Text modelling research has seen clear data increase to performance improvements relationships when training on large datasets.

For example:
* 16GB [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)
* 160GB [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
* 570GB [Language Models are Few-Shot Learners](../../permanent/language-models-are-few-shot-learners.md) (GPT-3)
* Finally, 1TB [Palm: Scaling language modeling with pathways](Palm:%20Scaling%20language%20modeling%20with%20pathways)

The authors are hoping to transfer this success to speech synthesis. So they propose a language model-based TTS framework, which can utilitse diverse multi-speaker speech data during training.

To synthesize personalised speech (e.g. [Zero-Shot TTS](../../permanent/zero-shot-tts.md)), they generate acoustic tokens on a 3-second recording, convert text into phonemes and use that as conditioning to predict speech codes.

The corresponding neural audio codec model can decode the speech codes.

This also means that advanced prompting-based large-model techniques (as in [GPT-3](../../permanent/gpt-3.md)) can be leveraged for the TTS tasks.

The acoustic tokens also allow us to generate diverse synthesized results in TTS by using different sampling strategies during inference.

They utilise [LibriLight](https://github.com/facebookresearch/libri-light) which has 60K hours of English speech with over 7000 unique speakers. They use a speech model to generate text transcriptions for all the audio.

The dataset has more noisy speech and inaccurate transcriptions than other TTS training datasets, like [LibriTTS](https://paperswithcode.com/dataset/libritts), but it has more diverse speakers and prosodies. The authors think this helps with robustness to noise and generalisability.

Table 1 summarises the difference between VALL-E and previous TTS systems.

![](../../../../_media/neural-codec-language-models-are-zero-shot-text-to-speech-synthesizers-table-1.png)

They evaluate VALL-E on datasets where all test speakers are unseen in training corpus, namely:
- [LibriSpeech](https://ieeexplore.ieee.org/document/7178964)
- [VCTK](https://datashare.ed.ac.uk/handle/10283/2651)

Results:
- Improve on SOTA TTS system [YourTTS](https://arxiv.org/abs/2112.02418) with improvements to speech naturalness
- LibriSpeech:
    - +0.12 [Comparitive Mean Opinion Score](Comparitive%20Mean%20Opinion%20Score) (CMOS), and speech naturalness
    - +0.93 [similarity-mean-option-score](../../../../permanent/similarity-mean-option-score.md) (SMOS).
- Beats the baseline on VCTK with +0.11 SMOS and +0.23 CMOS
- VCTK:
    - Gets +0.04 CMOS score against ground truth 
    - They claim the synthesied speech is an natural as the original recordings

Since VALL-E can synthesis diverse outputs with the same text and target speaker, it could be used for pseudo-data creation.

VALL-E was also found to keep the emotion of the acoustic prompt, and the environment (reverberation etc).

## 2. Related Work

### Zero-Shot TTS

There are two main categories of TTS solutions:

1. Cascading TTS Systems - these leverage a pipeline with an itermediate representation, typically a [Mel Spectrogram](../../permanent/melspectrogram.md), which is later decoded by a Vocoder.
2. End-to-End Methods - these model text directly to audio.

VALL-E is a cascading solution, which utilises discrete codes instead of Mel Spectrograms as the intermediate representation.

#### Cascaded TTS Methods

[Cascaded TTS Systems](../../../../permanent/cascaded-tts-systems.md) use an acoustic model and a vocoder, with Mel Spectrograms as the intermediate representation. Zero-shot multi-speaker TTS technic has mostly been explored via cascade methods.

One of the pioneering approaches, [Neural voice cloning with a few samples](Neural%20voice%20cloning%20with%20a%20few%20samples), proposes speaker adaptation and speaker encoding approaches.

These papers aim to improve the efficiency using less target data and speaker-specific params:
 
-  [Sample efficient adaptive text-to-speech](Sample%20efficient%20adaptive%20text-to-speech)
- [Spoken content and voice factorization for few-shot speaker adaptation](Spoken%20content%20and%20voice%20factorization%20for%20few-shot%20speaker%20adaptation)
- [Sample efficient adaptive text-to-speech](Sample%20efficient%20adaptive%20text-to-speech)

This paper [Meta-tts: Metalearning for few-shot speaker adaptive text-to-speech](Meta-tts:%20Metalearning%20for%20few-shot%20speaker%20adaptive%20text-to-speech) applies meta-learning on speaker adaptation, which only requires 5-shot to build a well-performed system.

Also, speaker encoding-based methods achieved great progress in recent years.

Speaker encoding based systems use a speaker encoder and a TTS component conditioned on that. A number of papers have show that a model can generate high-quality outputs with 3 seconds enrolled recordings for in-domain speakers:
- [Transfer learning from speaker verification to multispeaker text-to-speech synthesis](Transfer%20learning%20from%20speaker%20verification%20to%20multispeaker%20text-to-speech%20synthesis)
- [Neural voice cloning with a few samples](Neural%20voice%20cloning%20with%20a%20few%20samples)

Some papers have improved the quality of [Out-of-Domain](../../permanent/out-of-domain-data.md) speakers, using speaker embedding models, but it is still undesirable, according to this paper: [A survey on neural speech synthesis](A%20survey%20on%20neural%20speech%20synthesis)

Also, some Diffusion model based TTS was extended to zero-shot TTS and achieved good results:
- [Any-speaker adaptive text-to-speech synthesis with diffusion models.](Any-speaker%20adaptive%20text-to-speech%20synthesis%20with%20diffusion%20models.)

##### Key Comparisons

DALL-E follows a cascaded TTS approach, using an audio codec as intermediate representation.

It has [In-Context Learning](../../permanent/in-context-learning.md) capability, like GPT-3, but doesn't need fine-tuning, pre-designed features or a complex speaker encoder.

### Spoken generative pre-trained models

Self-supervised learning is widely used in speech understanding, for example:
- [A framework for self-supervised learning of speech representations](A%20framework%20for%20self-supervised%20learning%20of%20speech%20representations)
- [wav2vec 2.0: A framework for self-supervised learning of speech representations](wav2vec%202.0:%20A%20framework%20for%20self-supervised%20learning%20of%20speech%20representations)
- [Self-supervised speech representation learning by masked prediction of hidden units](Self-supervised%20speech%20representation%20learning%20by%20masked%20prediction%20of%20hidden%20units)
- [Wavlm: Large-scale self-supervised pre-training for full stack speech processing](Wavlm:%20Large-scale%20self-supervised%20pre-training%20for%20full%20stack%20speech%20processing)

And speech-to-speech generation:
* [Generative spoken language modeling from raw audio](Generative%20spoken%20language%20modeling%20from%20raw%20audio)
* [Audiolm: a language modeling approach to audio generation](Audiolm:%20a%20language%20modeling%20approach%20to%20audio%20generation)

A big topic is how to generate speech when you have no text. There are a few papers:
- [GSLM](GSLM) proposes to synthesise speech based on HuBERT codes.
* [Speech resynthesis from discrete disentangled self-supervised representations](Speech%20resynthesis%20from%20discrete%20disentangled%20self-supervised%20representations)
    - Improves the performance by combining HuBERT codes with codes of VQVAE and a speaker encoder.

[AudioLM](../../permanent/audiolm.md) follows a similar way but uses audio codecs to synthesize speech, together with semantic codes.

They note that AudioLM is able to synthesise speech based on audio codecs without training an additional vocoder such as [hifigan](../../../../permanent/hifigan.md).

AudioLM is a speech-to-speech model, whereas VALL-E is a TTS model, which gives specific control over the contents.

Another direction is to apply pre-training to the neural TTS  pre-trains speech decoder in TTS through autoregressive mel-spectrogram prediction

In paper [Unified-modal encoder-decoder pre-training for spoken language processing](Unified-modal%20encoder-decoder%20pre-training%20for%20spoken%20language%20processing) the authors propose a unified-modal encoder-decoder framework SpeechT5, which can leverage unlabelled speech and text data to pre-train all components of TTS model

l. Tjandra et al. [2019] quantizes unlabeled speech into discrete tokens by a VQVAE model [van den Oord et al., 2017], and train a model with the token-to-speech sequence.

They demonstrate that the pre-trained model only requires a small amount of real data for fine-tuning. Bai et al. [2022] proposes mask and reconstruction on mel spectrogram and showing better performance on speech editing and synthesis.

#### Key Comparison

Previous TTS pre-training work leverages less than 1K hours of data, whereas VALL-E is pre-trained with 60K hours of data

Furthermore, VALL-E is the first to use audio codec codes as intermediate representations, and emerge in-context learning capability in zero-shot TTS

## 3. Background: Speech Quantisation

Modelling raw audio is difficult: audio [Sample Rate](../../permanent/sample-rate.md) are generally is in the tens of thousands, and the [Bit Depth](../../permanent/bit-depth.md) is often 16-bit, requiring $2^{16} = 65,536$ probabilities per timestep to synthesise.

Speech [Quantisation](../../../../permanent/quantisation.md) is required to compress integer values and sequence length.

One approach that's commonly used is [Mu-Law Algorithm](../../../../permanent/mu-law-algorithm.md), which can quantise each time step to 256 values and reconstruct high-quality audio. Used in models like [Wavenet](../../permanent/wavenet.md). It does not help much with inference speed as it doesn't reduce sequence length.

Recently, [Vector Quantisation](../../../../permanent/vector-quantisation.md) has been used in self-supervised speech models for feature extraction, such as:
* [vq-wav2vec: Self-supervised learning of discrete speech representations](https://arxiv.org/abs/1910.05453)
* [HuBERT](../../permanent/hubert.md)

These papers show the codes from these self-supervised models can be used to reconstruct audio much faster than Wavenet:
* [Generative spoken language modeling from raw audio](https://arxiv.org/abs/2102.01192)
* [VQTTS: high-fidelity text-to-speech synthesis with self-supervised VQ acoustic feature](https://arxiv.org/abs/2204.00768)

However, the reconstruction quality is low and speaker identity is lost.

[AudioLM](../../permanent/audiolm.md) uses k-means tokens from a self-supervised model, but also uses acoustic tokens from a neural codec model, resulting in high-quality speech-to-speech. This paper also uses a neural codec model.

Codec models can reconstruct high-quality speech from discrete codes even if speaker in audio isn't in training, they work at low bitrates and capture recording conditions and other info in the audio.

Therefore, the authors use [Encodec](../../../../permanent/encodec.md) as tokeniser, which can input and output 24 kHz audio across variable bitrates. Can produce embedding at 75 Hz for input waves at 24 kHz a 320-fold reduction in sampling rate.

They use 8 quantisers with 1024 code dimensionality.

![](../../../../_media/neural-codec-language-models-are-zero-shot-text-to-speech-synthesizers-fig-2.png)

For a 10-second audio waveform, the discrete representation would be $750 \times 8$ ($750 = \frac{24000 \times 10}{320}$), which for EnCodec, gives us a 6k bitrate for 24 kHz audio reconstruction. More quantisers give better reconstruction quality.

## 4. VALL-E

### Problem formulation: Regarding TTS as Conditional Codec Language Modelling

Given a dataset $D = \{x_i, y_i\}$ where:
- $\mathbf{y}$ is an audio sample
- $x = \{x_0, x_1, ..., x_L\}$ is the corresponding phoneme transcription

They use [Encodec](../../../../permanent/encodec.md) to encode audio sample into discrete acoustic codes, denoted as $\text{Encodec}(y) = C^{T \times 8}$ where:
- C represents the 2-dimensional acoustic code matrix, and T is the downsampled utterance length.

The row vector of each acoustic code matrix $c_{t,:}$ represents the eight codes for frame t and the column vector of each acoustic code matrix $c_{:,j}$ represents the code sequence from the j-th codebook, where $j \in \{1, . . . , 8\}$

After quantization, the neural codec decoder is able to reconstruct the waveform, denoted as $Decodec(C) ≈ yˆ$.

Zero-shot TTS is about synthesising good speech data for speakers you didn't train on.

They treat the zero-shot TTS problem as a conditional codec language modelling task.

They train a neural language model to generate an acoustic code matrix $C$ conditioned on a phoneme sequence $x$ and an acoustic prompt matrix $\tilde{C}^{T' \times 8}$ with the optimisation objective of max $p(C|x, \tilde{C})$ (maximise probability of correctly generating the acoustic code matrix, given phoneme sequence and modified acoustic prompt matrix).

Here, $\tilde{C}$ is obtained by the same neural codec with an enrolled recording as the input.

They expect the neural language model learns to extract the content and speaker information from the phoneme sequence and the acoustic prompt, respectively.

At inference time, they have phoneme sequence, and 3-seconds of recording of unseen speaker.

They use that to estimate the code sequence, then use the Encodec decoder to synthesise into speech.

### 4.2 Training: Conditional Codec Language Modeling

Because of Residual Quantisation in Encodec, the tokens have a hierarchical structure: tokens from previous quantisers recover acoustic properties like speaker identity, while consecutive quantisers learn fine acoustic details. Each quantiser is trained to model residual from the previous quantiser, we they design two conditional language models in hierarchical manner.

For the discrete tokens from the first quantiser $c_{:,1}$, we train an [Autoregressive](Autoregressive) (AR) decoder-only language model.

Conditioned on the phoneme sequence $x$ and the acoustic prompt $\tilde{C}_{:,1}$. Formula:

$p(c_{:,1}|x, \tilde{C}_{:,1}; \theta AR) = \prod\limits^{T}_{t=0} p(c_{t,1}|c < t, 1 ,\tilde{c}_{:,1}, x; θAR)$

Since VALL-E is a decoder-only LM, the concatenation of $\tilde{c}_{: 1}$ and $c_{: 1}$ is a whole sequence, and they do not distinguish them or insert a specific token in training. Only $c_{:,1}$ is predicted while the prefix $\tilde{c}_{:,1}$ is given during inference

I think in other words they mean: they concat the speaker identity and the speech output to train on using an autoregressive approach. At inference time, they just provide the speaker identity code sequence.

Then, for the discrete tokens from the second to the last quantisers, $c_{:,j} \in [2,8]$, we train a [Non-Autoregressive](Non-Autoregressive) (NAR) language model.

Since the tokens can not access each other in a NAR manner, to constrain the speaker identity, the acoustic prompt matrix $\tilde{C}$ is used as an acoustic prompt. Thus, the model is conditioned on the phoneme sequence x, the acoustic prompt $\tilde{C}$ and the predicted acoustic tokens
belong to the previous codebooks $C_{:,<j}$

The combination of the AR model and the NAR model provides a good trade-off between speech quality and inference speed:

- On the one hand, the rate of the generated speech should be consistent with the enrolled recording, and it is hard to train a length predictor for different speakers since their speaking speed may be very diverse. In this case, the AR model is a more natural choice with its flexibility for acoustic sequence length prediction.
- On the other hand, for the consecutive stages, as the number of output slots follows the sequence length of the first stage, NAR can reduce the time complexity from O(T) to O(1).

Overall, the prediction of C can be modelled as:

$p(C|x, \tilde{C}; θ) = p(c_{:,1}|\tilde{C}_{:,1}, \mathbf{X}; \theta AR) \prod\limits_{j=2}^{8} p(\mathbf{c}_{:,j}|\mathbf{c}_{:,<j},\mathbf{x}, \tilde{C}; \theta NAR)$

### 4.2.1 Autoregressive Codec Language Modeling

The autoregressive language model generates the tokens from the first quantizer.

It comprises:
- phoneme embedding $Wx$
- an acoustic embedding $Wa$
- a transformer decoder
- a prediction layer

In order to generate speech with specific content, we use the phoneme sequence as the phoneme prompt of the language model.

Thus, the model input is the concatenation of $\mathbf{x}$ and $\mathbf{c}_{: 1}$, and two special `<EOS>` tokens are appended after each of them.

We compute sinuous position embedding separately for prompt and input tokens.

For the causal transformer model, each token $ct,1$ can attend to $(x, c≤t,1)$ as illustrated in the left part of Figure 3.

![](../../../../_media/neural-codec-language-models-are-zero-shot-text-to-speech-synthesizers-fig3.png)
*Figure 3 from Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers*

The model is optimised to maximise the probability of the next token in the first codebook.

They share the parameters of the output projection layer with the parameters of the acoustic embedding $Wa$

In the $AR$ model, they don't  explicitly extract an audio clip as the prompt in training.

The training process is pure casual language model training.

In this way, any prefix sequence $\mathbf{c}_{< t, 1}$ is treated as a prompt for the latter part of the sequence $\mathbf{c}_{\ge t, 1}$. During inference, given an enrolled recording, we should concatenate the phoneme sequence of the enrolled recording and the phoneme sequence for synthesis together.

Meanwhile, the acoustic token sequence of the enrolled recording is used as the prefix AR decoding, as formulated in equation 1. We will study the superiority of this setting in the experiment.

### 4.2.2 Non-Autoregressive Codec Language Modelling

When we obtain the first quantiser codes by the AR model, we employ a non-autoregressive (NAR) model to generate codes of the other seven quantisers.

The NAR model has a similar architecture to the AR model, except that it contains eight separate acoustic embedding layers.

In each training step, we randomly sample a training stage $i \in [2, 8]$

The model is trained to maximize the acoustic tokens from the i-th quanitzer codebook.

The acoustic tokens from state 1 to stage $i - 1$ are embedded and summed up as model input:

$e_{c_{t,j}} = W^{j}_{a} \odot c_{t,j}$
$\mathbf{e_ct} = \sum\limits^{i-1}_{j=1} e_{c_{t,j}}$

where $\odot$ indicates index selection.

The phoneme sequence is also regarded as the prompt of the language model.

Besides, to clone the unique voice of the given speaker, we also use the acoustic tokens from the enrolled speech as the acoustic prompt. Specifically, we first tokenise the enrolled speech with the neural codec model as $\mathbf{\tilde{C}}^{T \times 8}$.

The embedded representations from all of the eight codebooks are summed up as the acoustic prompt, $\mathbf{e}_{\tilde{c}_{t_j}} = \sum_{j=1}^{8} e_{\tilde{c}_{:,<1}}$

To predict the acoustic tokens from the $i-th$ codebook, the transformer input is the concatenation of $(\mathbf{e_x}, \mathbf{e_{\tilde{c}}}, \mathbf{c_{:,<i}})$

The positional embeddings are also computed separately for prompts and the acoustic sequence.

The current stage $i$ is injected into the network with [Adaptive Layer Normalization](../../../../permanent/Adaptive%20Layer%20Normalization.md) operator, i.e., $\text{AdaLN}(h, i) = a_i \ \text{LayerNorm}(h) + b_i$, where h is the intermediate activations, $ai$ and $bi$ are obtained from a linear projection of the stage embedding. Unlike AR, the NAR model allows each token to attend to all the input tokens in the self-attention layer.

We also share the parameters of the acoustic embedding layer and the output prediction layer.

This means the weights of the $j-th$ prediction layer are the same as the $(j+1)$-th acoustic embedding layer.

### 4.3 Inference: In-Context Learning via Prompting

[In-Context Learning](../../permanent/in-context-learning.md) is an ability of text-based language models, which can predict labels for unseen inputs without fine-tuning.

The model is believe to have in-context learning capability.

However, the in-context learning capability of existing TTS systems is not strong, because they either require additional fine-tuning or degrade dramatically for unseen speakers.

For language models, prompting is necessary to enable in-context learning in the zero-shot scenario. We design prompts and inference as follows.

We first convert the text into a phoneme sequence and encode the enrolled recording into an acoustic matrix, forming the phoneme prompt and acoustic prompt.

Both prompts are used in the AR and NAR models.
 
For the AR model, we use sampling-based decoding conditioned on the prompts since we observe that beam search may lead the LM into an infinity loop.

Furthermore, the sampling-based method could significantly increase the diversity of the output.

For the NAR model, we use greedy decoding to choose the token with the highest probability

Finally, we use the neural codec decoder to generate the waveform conditioned on the eight code sequences. The acoustic prompt may or may not semantically relate to the speech to be synthesized, resulting in two cases:

**VALL-E**: main interest is generating content for unseen speakers. The model is given a text sentence, a segment of enrolled speech, and its corresponding transcription. We prepend the transcription phoneme of the enrolled speech to the phonemem sequence of the given sentence as the phoneme prompt, and use the first layer acoustic token of the enrolled speech $\tilde{c_{:,1}}$ as acoustic prefix. With the phoneme prompt and the acoustic prefix, VALL-E generates the acoustic tokens for the given text cloning the voice of this speaker.

**VALL-E-continual**: In this setting, we use the whole transcription and the first 3 seconds of the utterance as the phoneme and acoustic prompts respectively, and ask the model to generate the continuations. The inference process is the same as setting VALL-E, except that the enrolled speech and the generated speech are semantically continuous.

## Experiment

### Experiment Setup

#### Dataset

LibriLight
- 60K hours of unlabelled speech from English audiobooks.
- Number of distinct speakers is around 7k.
- Annotations:
    - Use [[Hybrid Deep Neural Network--Hidden Markov Model]] model for speech recognition, training on 960 hours of labelled LirbisSpeech, following the Kaldi recipe from [The kaldi speech recognition toolkit](https://www.danielpovey.com/files/2011_asru_kaldi.pdf)
- Once hybrid is trained, unlabelled speech data is decoded and trasnduced to the best phoneme-level alignment paths where the frameshift is 30ms. The EnCodec model is used to generate the acoustic code matrix for the 60K hours of data.

#### Model

Both the AR model and the NAR model have the same transformer architecture:
- 12 layers
- 16 attention heads
- an embedding dimension of 1024
- a feed-forward layer dimension of 4096
- dropout of 0.1.

The average length of the waveform in LibriLight is 60 seconds.

During training, they randomly crop waveform to random length between 10 seconds and 20 seconds.

The corresponding phoneme alignments are used as the phoneme prompt.

Remove consecutive repetitions in the force-aligned phoneme sequence. For the NAR acoustic prompt tokens, we select a random segment waveform of 3 seconds from the same utterance.

The models are trained using 16 6 NVIDIA TESLA V100 32GB GPUs with a batch size of 6k acoustic tokens per GPU for 800k steps.

We optimise the models with the AdamW optimiser and warn up the learning rate for the first 32k update to a peak of 5x10^{4}, then linear decay.

### Baseline

SOTA zero-shot TTS model YourTTS. Trained on combined dataset of VCTK, LibriTTS and TTS-Portuguese. Use the released checkpoint.

### Automatic metrics:

- SOTA speaker verification model, [WavLM-TDNN] to evaluate speaker similarity between the prompt (the decompressed enrolled speech) and syntehsized speech.
- WavLM-TDNN achieved the top rank at the VoxSRC Challenge 2021 and 2022 leaderboards.
- It reached an average Equal Error Rate (EER) of 0.383, 0.480, and 0.986 on Vox1-O, Vox1-E, and Vox1-H respectively. 
- The similarity score predicted by WavLM-TDNN is in the range of [−1, 1], where a larger value indicates a higher similarity of input samples.
- We also evaluate the synthesis robustness of our model.
- Neural TTS systems suffer from the robustness issue, which sometimes has deletion, insertion, and replacement errors due to wrong attention alignments.
- We perform ASR on the generated audio and calculate the word error rate (WER) with respect to the original transcriptions.
- In this experiment, we employ the HuBERT-Large [Hsu et al., 2021] model fine-tuned on LibriSpeech 960h as the ASR model, which is a CTC-based model without language model fusion.

#### Human evaluation

We calculate the comparative mean option score (CMOS) and similarity mean option score (SMOS) by crowdsourcing, where 12 and 6 native speakers are invited as CMOS and SMOS contributors.

The scale of SMOS is from 1 to 5 with 0.5-point increments. CMOS ranges from -3 (the new system is much worse than baseline) to 3 (the new system is much better than baseline) with intervals of 1. CMOS is an indicator of speech naturalness, and SMOS measures whether the speech is similar to the original speaker’s voice.

#### 5.2 LibriSpeech Evaluation

- use LibriSpeech [Panayotov et al., 2015] for zero-shot TTS evaluation, since there is no speaker overlap between LibriLight training data and LibriSpeech test-clean data.
-  Following Borsos et al. [2022], we use the samples from LibriSpeech test-clean with lengths between 4 and 10 seconds, resulting in a 2.2 hours subset.
- For each sample synthesis, VALL-E randomly choose another utterance of the same speaker and crop a 3-seconds speech segment as the enrolled speech.
- Each experiment runs three times and the average score is reported. VALL-E-continual uses the first 3 seconds of the ground-truth speech as enrolled speech.
- Table 2 shows the objective evaluation results:
    - We first compute the WER score and the speaker similarity score of the ground truth speech as the upper bound. 
    - To compare the speaker similarity, we use speech pairs from the same speaker in the test set.
    - Compared with the YourTTS baseline, our model is significantly better in both robustness and speaker similarity; generated speech is highly faithful to the given text and the given enrolled speech.
    - Furthermore, the word error rate can be further reduced in VALL-E-continual setting, because the acoustic tokens for the first 3 seconds are extracted from the ground truth.
- They also compare robustness with otehr speech-tospeech LM-based generation models, GSLM and AudioLM, which use audio latent codecs as input. GSLMT use HuBERT code as input and reconstructs the waveform with [Tacotron 2](Tacotron%202) model and WaveGlow vocoder. We run their open-source dode using the relesad model and evaluate the results.
    






