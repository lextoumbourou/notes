{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00:56:22 - Language model from scratch\n\n* Going to learn how a recurrent neural network works.\n\n## 00:56:52 - Question: are there model interpretabilty tools for language models?\n\n* There are some, but won't be covered in this part of the course.\n\n## 00:58:11 - Preparing the dataset: tokenisation and numericalisation\n\n* Jeremy created a dataset called human numbers that contains first 10k numbers written out in English.\n  * Seems very few people create datasets, even though it's not particularly hard.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from fastai.text.all import *\nfrom fastcore.foundation import L\n\npath = untar_data(URLs.HUMAN_NUMBERS)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:40.807854Z","iopub.execute_input":"2021-08-28T01:05:40.808217Z","iopub.status.idle":"2021-08-28T01:05:44.513904Z","shell.execute_reply.started":"2021-08-28T01:05:40.808184Z","shell.execute_reply":"2021-08-28T01:05:44.512253Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"Path.BASE_PATH = path","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:44.516097Z","iopub.execute_input":"2021-08-28T01:05:44.516497Z","iopub.status.idle":"2021-08-28T01:05:44.521320Z","shell.execute_reply.started":"2021-08-28T01:05:44.516452Z","shell.execute_reply":"2021-08-28T01:05:44.520307Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"path.ls()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:44.523500Z","iopub.execute_input":"2021-08-28T01:05:44.523798Z","iopub.status.idle":"2021-08-28T01:05:44.538082Z","shell.execute_reply.started":"2021-08-28T01:05:44.523768Z","shell.execute_reply":"2021-08-28T01:05:44.536985Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(#2) [Path('valid.txt'),Path('train.txt')]"},"metadata":{}}]},{"cell_type":"code","source":"lines = L()\nwith open(path/'train.txt') as f:\n    lines += L(*f.readlines())\nwith open(path/'valid.txt') as f:\n    lines += L(*f.readlines())\n    \nlines","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:44.539578Z","iopub.execute_input":"2021-08-28T01:05:44.539846Z","iopub.status.idle":"2021-08-28T01:05:44.553942Z","shell.execute_reply.started":"2021-08-28T01:05:44.539821Z","shell.execute_reply":"2021-08-28T01:05:44.553026Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"},"metadata":{}}]},{"cell_type":"markdown","source":"* Concat them together and put a dot between them for tokenising.","metadata":{}},{"cell_type":"code","source":"text = ' . '.join([l.strip() for l in lines])\ntext[:100]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:44.555475Z","iopub.execute_input":"2021-08-28T01:05:44.555949Z","iopub.status.idle":"2021-08-28T01:05:44.570461Z","shell.execute_reply.started":"2021-08-28T01:05:44.555916Z","shell.execute_reply":"2021-08-28T01:05:44.568816Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"},"metadata":{}}]},{"cell_type":"markdown","source":"* Tokenise by splitting on spaces:","metadata":{}},{"cell_type":"code","source":"tokens = L(text.split(' '))\ntokens[100:110]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:44.571468Z","iopub.execute_input":"2021-08-28T01:05:44.571890Z","iopub.status.idle":"2021-08-28T01:05:44.586938Z","shell.execute_reply.started":"2021-08-28T01:05:44.571861Z","shell.execute_reply":"2021-08-28T01:05:44.586006Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(#10) ['.','forty','two','.','forty','three','.','forty','four','.']"},"metadata":{}}]},{"cell_type":"markdown","source":"* Create a vocab by getting unique tokens:","metadata":{}},{"cell_type":"code","source":"vocab = L(tokens).unique()\nvocab, len(vocab)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:44.588231Z","iopub.execute_input":"2021-08-28T01:05:44.588775Z","iopub.status.idle":"2021-08-28T01:05:44.605220Z","shell.execute_reply.started":"2021-08-28T01:05:44.588733Z","shell.execute_reply":"2021-08-28T01:05:44.604193Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"((#30) ['one','.','two','three','four','five','six','seven','eight','nine'...],\n 30)"},"metadata":{}}]},{"cell_type":"markdown","source":"* Convert tokens into numbers by looking up index of each word:","metadata":{}},{"cell_type":"code","source":"word2idx = {w: i for i,w in enumerate(vocab)}\nnums = L(word2idx[i] for i in tokens)\ntokens, nums","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:44.607647Z","iopub.execute_input":"2021-08-28T01:05:44.607940Z","iopub.status.idle":"2021-08-28T01:05:44.627317Z","shell.execute_reply.started":"2021-08-28T01:05:44.607913Z","shell.execute_reply":"2021-08-28T01:05:44.626331Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"((#63095) ['one','.','two','.','three','.','four','.','five','.'...],\n (#63095) [0,1,2,1,3,1,4,1,5,1...])"},"metadata":{}}]},{"cell_type":"markdown","source":"* That gives us a small easy dataset for building a language model.","metadata":{}},{"cell_type":"markdown","source":"## 01:01:31 - Language model from scratch\n\n* Create a independent and dependent pair: first 3 words are input, next is dependent.","metadata":{}},{"cell_type":"code","source":"L((tokens[i:i+3], tokens[i+3]) for i in range(0, len(tokens)-4, 3))","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:44.629583Z","iopub.execute_input":"2021-08-28T01:05:44.630180Z","iopub.status.idle":"2021-08-28T01:05:44.935751Z","shell.execute_reply.started":"2021-08-28T01:05:44.630134Z","shell.execute_reply":"2021-08-28T01:05:44.934564Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]"},"metadata":{}}]},{"cell_type":"markdown","source":"* Same thing numericalised:","metadata":{}},{"cell_type":"code","source":"seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0, len(nums)-4, 3))\nseqs","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:44.937488Z","iopub.execute_input":"2021-08-28T01:05:44.937919Z","iopub.status.idle":"2021-08-28T01:05:45.704843Z","shell.execute_reply.started":"2021-08-28T01:05:44.937876Z","shell.execute_reply":"2021-08-28T01:05:45.703879Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"},"metadata":{}}]},{"cell_type":"markdown","source":"* Can batch those with a `DataLoader` object\n  * Take first 80% as training, last 20% as validation.","metadata":{}},{"cell_type":"code","source":"bs = 64\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(seqs[:cut], seqs[:cut], bs=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:45.706138Z","iopub.execute_input":"2021-08-28T01:05:45.706426Z","iopub.status.idle":"2021-08-28T01:05:45.712325Z","shell.execute_reply.started":"2021-08-28T01:05:45.706398Z","shell.execute_reply":"2021-08-28T01:05:45.711350Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## 01:03:35 - Simple Language model","metadata":{}},{"cell_type":"markdown","source":"* 3 layer neural network.\n  * One linear layer that is reused 3 times.\n  * Each time the result is added to the embedding.","metadata":{}},{"cell_type":"code","source":"class LMModel(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2hidden = nn.Embedding(vocab_sz, n_hidden)\n        self.hidden2hidden = nn.Linear(n_hidden, n_hidden)\n        self.hidden2output = nn.Linear(n_hidden, vocab_sz)\n    \n    def forward(self, x):\n        hidden = self.input2hidden(x[:,0])\n        hidden = F.relu(self.hidden2hidden(hidden))\n    \n        hidden = hidden + self.input2hidden(x[:,1])\n        hidden = F.relu(self.hidden2hidden(hidden))\n    \n        hidden = hidden + self.input2hidden(x[:,2])\n        hidden = F.relu(self.hidden2hidden(hidden))\n\n        return self.hidden2output(hidden)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:45.713872Z","iopub.execute_input":"2021-08-28T01:05:45.714354Z","iopub.status.idle":"2021-08-28T01:05:45.726532Z","shell.execute_reply.started":"2021-08-28T01:05:45.714309Z","shell.execute_reply":"2021-08-28T01:05:45.725558Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 01:04:49 - Question: can you speed up fine-tuning the NLP model?\n\n* Do something else while you wait or leave overnight.\n\n## 01:05:44 - Simple Language model cont.","metadata":{}},{"cell_type":"markdown","source":"* 2 interesting happening:\n  * Some of the inputs are being fed into later layers, instead of just the first.\n  * The model is reusing hidden state throughout layers.","metadata":{}},{"cell_type":"code","source":"learn = Learner(dls, LMModel(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:45.727907Z","iopub.execute_input":"2021-08-28T01:05:45.728466Z","iopub.status.idle":"2021-08-28T01:05:57.949859Z","shell.execute_reply.started":"2021-08-28T01:05:45.728422Z","shell.execute_reply":"2021-08-28T01:05:57.948346Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.800355</td>\n      <td>1.929378</td>\n      <td>0.462197</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.364388</td>\n      <td>1.685311</td>\n      <td>0.466120</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.399541</td>\n      <td>1.464157</td>\n      <td>0.490787</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.362921</td>\n      <td>1.412893</td>\n      <td>0.495007</td>\n      <td>00:03</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"* We can find out if that accuracy is good by making a simple model that predicts most common token:","metadata":{}},{"cell_type":"code","source":"c = Counter(tokens[cut:])\nmc = c.most_common(5)\nmc","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:57.951340Z","iopub.execute_input":"2021-08-28T01:05:57.951647Z","iopub.status.idle":"2021-08-28T01:05:57.967424Z","shell.execute_reply.started":"2021-08-28T01:05:57.951614Z","shell.execute_reply":"2021-08-28T01:05:57.966501Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[('thousand', 7104),\n ('.', 7103),\n ('hundred', 6405),\n ('nine', 2440),\n ('eight', 2344)]"},"metadata":{}}]},{"cell_type":"code","source":"mc[0][1]/len(tokens[cut:])","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:57.968531Z","iopub.execute_input":"2021-08-28T01:05:57.968972Z","iopub.status.idle":"2021-08-28T01:05:57.982335Z","shell.execute_reply.started":"2021-08-28T01:05:57.968925Z","shell.execute_reply":"2021-08-28T01:05:57.981542Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0.15353028894988222"},"metadata":{}}]},{"cell_type":"markdown","source":"## 01:14:41 - Recurrent neural network","metadata":{}},{"cell_type":"markdown","source":"* We can refactor in Python using a for loop.\n  * Note that `hidden = 0` is being broadcast into the hidden state.","metadata":{}},{"cell_type":"code","source":"class LMModel2(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2hidden = nn.Embedding(vocab_sz, n_hidden)\n        self.hidden2hidden = nn.Linear(n_hidden, n_hidden)\n        self.hidden2output = nn.Linear(n_hidden, vocab_sz)\n    \n    def forward(self, x):\n        hidden = 0.\n        \n        for i in range(3):\n            hidden = hidden + self.input2hidden(x[:,i])\n            hidden = F.relu(self.hidden2hidden(hidden))\n\n        return self.hidden2output(hidden)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:57.983695Z","iopub.execute_input":"2021-08-28T01:05:57.983986Z","iopub.status.idle":"2021-08-28T01:05:57.994675Z","shell.execute_reply.started":"2021-08-28T01:05:57.983959Z","shell.execute_reply":"2021-08-28T01:05:57.993679Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:05:57.995935Z","iopub.execute_input":"2021-08-28T01:05:57.996322Z","iopub.status.idle":"2021-08-28T01:06:10.460681Z","shell.execute_reply.started":"2021-08-28T01:05:57.996273Z","shell.execute_reply":"2021-08-28T01:06:10.459492Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.827151</td>\n      <td>1.976822</td>\n      <td>0.457739</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.383131</td>\n      <td>1.667376</td>\n      <td>0.468141</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.398363</td>\n      <td>1.497497</td>\n      <td>0.490252</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.382042</td>\n      <td>1.432224</td>\n      <td>0.490965</td>\n      <td>00:03</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"* That's what a Recurrent Neural Network is!","metadata":{}},{"cell_type":"markdown","source":"## 01:18:39 - Improving the RNN\n\n* Note that we're setting the previous state to 0.\n  * However, the hidden state from sequence to sequence contains useful information.\n  * We can rewrite to maintain state of RNN.","metadata":{}},{"cell_type":"code","source":"class LMModel3(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2hidden = nn.Embedding(vocab_sz, n_hidden)\n        self.hidden2hidden = nn.Linear(n_hidden, n_hidden)\n        self.hidden2output = nn.Linear(n_hidden, vocab_sz)\n        self.hidden = 0.\n        \n    \n    def forward(self, x):\n        for i in range(3):\n            self.hidden = self.hidden + self.input2hidden(x[:,i])\n            self.hidden = F.relu(self.hidden2hidden(self.hidden))\n    \n        output = self.hidden2output(self.hidden)\n        self.hidden = self.hidden.detach()\n\n        return output\n    \n    def reset(self):\n        self.h = 0.","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:06:10.462296Z","iopub.execute_input":"2021-08-28T01:06:10.462682Z","iopub.status.idle":"2021-08-28T01:06:10.470693Z","shell.execute_reply.started":"2021-08-28T01:06:10.462646Z","shell.execute_reply":"2021-08-28T01:06:10.469808Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## 01:19:41 - Back propagation through time\n\n* Note that we called `self.hidden.detach()` each forward pass to ensure we're not back propogating through all the previous forward passes.\n  * Known as Back propagation through time (BPTT)\n  \n## 01:22:19 - Ordered sequences and callbacks\n\n* Samples must be seen in correct order - each batch needs to connect to previous batch.\n* At the start of each epoch, we need to call reset.","metadata":{}},{"cell_type":"code","source":"def group_chunks(ds, bs):\n    m = len(ds) // bs\n    new_ds = L()\n    for i in range(m):\n        new_ds += L(ds[i + m*j] for j in range(bs))\n    return new_ds\n\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(\n    group_chunks(seqs[:cut], bs),\n    group_chunks(seqs[cut:], bs),\n    bs=bs, drop_last=True, shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:06:10.471744Z","iopub.execute_input":"2021-08-28T01:06:10.472230Z","iopub.status.idle":"2021-08-28T01:06:10.552298Z","shell.execute_reply.started":"2021-08-28T01:06:10.472180Z","shell.execute_reply":"2021-08-28T01:06:10.551093Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"* At start of epoch, we call `reset`\n  * Last thing to add is little tweak of training model via a`Callback` called `ModelReseter`","metadata":{}},{"cell_type":"code","source":"from fastai.callback.rnn import ModelResetter","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:06:10.553613Z","iopub.execute_input":"2021-08-28T01:06:10.553917Z","iopub.status.idle":"2021-08-28T01:06:10.558269Z","shell.execute_reply.started":"2021-08-28T01:06:10.553887Z","shell.execute_reply":"2021-08-28T01:06:10.557302Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(10, 3e-3)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:06:10.559693Z","iopub.execute_input":"2021-08-28T01:06:10.560003Z","iopub.status.idle":"2021-08-28T01:06:34.176422Z","shell.execute_reply.started":"2021-08-28T01:06:10.559974Z","shell.execute_reply":"2021-08-28T01:06:34.175584Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.708671</td>\n      <td>1.819227</td>\n      <td>0.483173</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.231601</td>\n      <td>1.618543</td>\n      <td>0.513221</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.094240</td>\n      <td>1.709943</td>\n      <td>0.462019</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.020911</td>\n      <td>1.706639</td>\n      <td>0.529808</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.977250</td>\n      <td>1.659540</td>\n      <td>0.542788</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.909005</td>\n      <td>1.681440</td>\n      <td>0.568750</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.873126</td>\n      <td>1.604766</td>\n      <td>0.578125</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.811297</td>\n      <td>1.745289</td>\n      <td>0.600000</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.774836</td>\n      <td>1.684849</td>\n      <td>0.606250</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.762098</td>\n      <td>1.629690</td>\n      <td>0.618510</td>\n      <td>00:02</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"* Training is doing a lot better now.","metadata":{}},{"cell_type":"markdown","source":"## 01:25:00 - Creating more signal\n\n* Instead of putting output stage outside the loop, can put it in the loop.\n  * After every hidden state, we get a prediction.\n* Can we change the data so dependant variable has each of the three words after each three input words.","metadata":{}},{"cell_type":"code","source":"sl = 16\nseqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n        for i in range(0, len(nums) - sl - 1, sl))\ncut = int(len(seqs) * 0.8)\n\ndls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n                             group_chunks(seqs[cut:], bs),\n                             bs=bs, drop_last=True, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:06:34.179248Z","iopub.execute_input":"2021-08-28T01:06:34.179592Z","iopub.status.idle":"2021-08-28T01:06:34.839139Z","shell.execute_reply.started":"2021-08-28T01:06:34.179559Z","shell.execute_reply":"2021-08-28T01:06:34.837971Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"[L(vocab[o] for o in s) for s in seqs[0]]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:06:34.842128Z","iopub.execute_input":"2021-08-28T01:06:34.842458Z","iopub.status.idle":"2021-08-28T01:06:34.852320Z","shell.execute_reply.started":"2021-08-28T01:06:34.842429Z","shell.execute_reply":"2021-08-28T01:06:34.851092Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n (#16) ['.','two','.','three','.','four','.','five','.','six'...]]"},"metadata":{}}]},{"cell_type":"markdown","source":"* We can modify the model to return a list of outputs.","metadata":{}},{"cell_type":"code","source":"class LMModel4(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2hidden = nn.Embedding(vocab_sz, n_hidden)\n        self.hidden2hidden = nn.Linear(n_hidden, n_hidden)\n        self.hidden2output = nn.Linear(n_hidden, vocab_sz)\n        self.hidden = 0.\n\n    def forward(self, x):\n        outputs = []\n\n        for i in range(sl):\n            self.hidden = self.hidden + self.input2hidden(x[:,i])\n            self.hidden = F.relu(self.hidden2hidden(self.hidden))\n            outputs.append(self.hidden2output(self.hidden))\n\n        self.hidden = self.hidden.detach()\n\n        return torch.stack(outputs, dim=1)\n    \n    def reset(self):\n        self.h = 0.","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:06:34.854487Z","iopub.execute_input":"2021-08-28T01:06:34.855097Z","iopub.status.idle":"2021-08-28T01:06:34.867242Z","shell.execute_reply.started":"2021-08-28T01:06:34.855033Z","shell.execute_reply":"2021-08-28T01:06:34.865993Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"* We have to write a custom loss function to flatten the outputs:","metadata":{}},{"cell_type":"code","source":"def loss_func(inp, targ):\n    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:06:34.868853Z","iopub.execute_input":"2021-08-28T01:06:34.869418Z","iopub.status.idle":"2021-08-28T01:06:34.884207Z","shell.execute_reply.started":"2021-08-28T01:06:34.869378Z","shell.execute_reply":"2021-08-28T01:06:34.883191Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:06:34.885909Z","iopub.execute_input":"2021-08-28T01:06:34.886507Z","iopub.status.idle":"2021-08-28T01:06:51.266256Z","shell.execute_reply.started":"2021-08-28T01:06:34.886461Z","shell.execute_reply":"2021-08-28T01:06:51.264970Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3.248546</td>\n      <td>3.115103</td>\n      <td>0.164551</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.386219</td>\n      <td>1.933674</td>\n      <td>0.466797</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.760738</td>\n      <td>1.806952</td>\n      <td>0.472331</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.467624</td>\n      <td>1.882069</td>\n      <td>0.503092</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.259531</td>\n      <td>1.883375</td>\n      <td>0.535889</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.079385</td>\n      <td>2.010900</td>\n      <td>0.555339</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.939819</td>\n      <td>1.925642</td>\n      <td>0.572917</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.831508</td>\n      <td>1.877170</td>\n      <td>0.591390</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.748246</td>\n      <td>2.093921</td>\n      <td>0.574544</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.673372</td>\n      <td>2.032348</td>\n      <td>0.602865</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.628112</td>\n      <td>2.123352</td>\n      <td>0.600667</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.594529</td>\n      <td>2.048167</td>\n      <td>0.618001</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.556409</td>\n      <td>2.081254</td>\n      <td>0.619466</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.534996</td>\n      <td>2.098750</td>\n      <td>0.619059</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.524235</td>\n      <td>2.071640</td>\n      <td>0.617269</td>\n      <td>00:01</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 01:28:29 - Multilayer RNN\n\n* Even though the RNN seemed to have a lot of layers, each layer is sharing the same weight matrix.\n  * Not that much better than a simple linear model.\n* A multilayer RNN can stake multiple linear layers within the for loop.\n* PyTorch provides the `nn.RNN` class for creating multilayers RNNs.","metadata":{}},{"cell_type":"code","source":"class LMModel5(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.input2hidden = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.hidden2output = nn.Linear(n_hidden, vocab_sz)\n        self.hidden = torch.zeros(n_layers, bs, n_hidden)\n\n    def forward(self, x):\n        res, h = self.rnn(self.input2hidden(x), self.hidden)\n        self.hidden = h.detach()\n        return self.hidden2output(res)\n    \n    def reset(self):\n        self.hidden.zero_()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:06:51.268160Z","iopub.execute_input":"2021-08-28T01:06:51.268610Z","iopub.status.idle":"2021-08-28T01:06:51.277174Z","shell.execute_reply.started":"2021-08-28T01:06:51.268560Z","shell.execute_reply":"2021-08-28T01:06:51.275995Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"learn = Learner(dls, LMModel5(len(vocab), 64, 2), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:06:51.278666Z","iopub.execute_input":"2021-08-28T01:06:51.279100Z","iopub.status.idle":"2021-08-28T01:07:08.123712Z","shell.execute_reply.started":"2021-08-28T01:06:51.279056Z","shell.execute_reply":"2021-08-28T01:07:08.122660Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3.041824</td>\n      <td>2.657038</td>\n      <td>0.445638</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.138109</td>\n      <td>1.801325</td>\n      <td>0.470947</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.706817</td>\n      <td>1.811761</td>\n      <td>0.436361</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.530675</td>\n      <td>1.894353</td>\n      <td>0.373617</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.375850</td>\n      <td>1.788797</td>\n      <td>0.450928</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.222879</td>\n      <td>1.682562</td>\n      <td>0.478353</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.038785</td>\n      <td>1.689814</td>\n      <td>0.497152</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.894064</td>\n      <td>1.736756</td>\n      <td>0.507975</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.791512</td>\n      <td>1.795761</td>\n      <td>0.511068</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.720978</td>\n      <td>1.885496</td>\n      <td>0.516113</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.670370</td>\n      <td>1.939440</td>\n      <td>0.519043</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.633727</td>\n      <td>1.986372</td>\n      <td>0.519613</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.609256</td>\n      <td>2.031443</td>\n      <td>0.520345</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.594163</td>\n      <td>2.023642</td>\n      <td>0.519775</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.585938</td>\n      <td>2.040995</td>\n      <td>0.518392</td>\n      <td>00:01</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"* The model seems to be doing worse. Validation loss appears to be really bad now.\n  \n## 01:32:39 - Exploding and vanishing gradients \n\n* Very deep models can be hard to train due to exploding or vanishing gradients.\n* Doing a lot of matrix multiplications across layers can give you very big or very small results.\n  * This can also cause gradients to grow.\n* This is because numbers in a computer aren't stored precisely: they're stored as floating point numbers.\n  * Really big or small numbers become very close together and differences become practically 0.\n* Lots of ways to deal with this:\n  * Batch norm.\n  * Smart initialisation.\n* One simple technique is to use an RNN architecture called LSTM.\n\n## 01:36:29 - LSTM\n\n* Designed such that there's mini neural networks that decide how much previous state should be kept or discarded.\n* Main detail: can replace matrix multiplication with LSTMCell sequence below:","metadata":{"execution":{"iopub.status.busy":"2021-08-25T13:05:31.122355Z","iopub.execute_input":"2021-08-25T13:05:31.122799Z","iopub.status.idle":"2021-08-25T13:05:31.129885Z","shell.execute_reply.started":"2021-08-25T13:05:31.122748Z","shell.execute_reply":"2021-08-25T13:05:31.12832Z"}}},{"cell_type":"code","source":"class LSTMCell(nn.Module):\n    def __init__(self, num_inputs, num_hidden):\n        self.forget_gate = nn.Linear(num_inputs + num_hidden, num_hidden)\n        self.input_gate = nn.Linear(num_inputs + num_hidden, num_hidden)\n        self.cell_gate = nn.Linear(num_inputs + num_hidden, num_hidden)\n        self.output_gate = nn.Linear(num_inputs + num_hidden, num_hidden)\n        \n    def forward(self, input, state):\n        h, c = state\n        h = torch.stack([h, input], dim=1)\n        forget = torch.sigmoid(self.forget_gate(h))\n        c = c * forget\n        inp = torch.sigmoid(self.input_gate(h))\n        cell = torch.tanh(self.cell_gate(h))\n        c = c + inp * cell\n        outgate = torch.sigmoid(self.output_gate(h))\n        h = outgate * torch.tanh(c)\n        return h, (h, c)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:18:54.242905Z","iopub.execute_input":"2021-08-28T01:18:54.243286Z","iopub.status.idle":"2021-08-28T01:18:54.252524Z","shell.execute_reply.started":"2021-08-28T01:18:54.243254Z","shell.execute_reply":"2021-08-28T01:18:54.251053Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"* RNN that uses LSTMCell is called `LSTM`","metadata":{}},{"cell_type":"code","source":"class LMModel6(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.input2hidden = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.hidden2output = nn.Linear(n_hidden, vocab_sz)\n        self.hidden = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n\n    def forward(self, x):\n        res, h = self.rnn(self.input2hidden(x), self.hidden)\n        self.hidden = [h_.detach() for h_ in h]\n        return self.hidden2output(res)\n    \n    def reset(self):\n        for h in self.hidden:\n            h.zero_()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:21:55.949627Z","iopub.execute_input":"2021-08-28T01:21:55.950151Z","iopub.status.idle":"2021-08-28T01:21:55.957927Z","shell.execute_reply.started":"2021-08-28T01:21:55.950115Z","shell.execute_reply":"2021-08-28T01:21:55.956628Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"learn = Learner(dls, LMModel6(len(vocab), 64, 2), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 1e-2)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:21:56.193017Z","iopub.execute_input":"2021-08-28T01:21:56.193473Z","iopub.status.idle":"2021-08-28T01:22:24.398182Z","shell.execute_reply.started":"2021-08-28T01:21:56.193435Z","shell.execute_reply":"2021-08-28T01:22:24.397152Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3.012093</td>\n      <td>2.717043</td>\n      <td>0.217204</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.218240</td>\n      <td>2.029626</td>\n      <td>0.351644</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.658893</td>\n      <td>1.942375</td>\n      <td>0.456868</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.369292</td>\n      <td>2.049241</td>\n      <td>0.499268</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.129897</td>\n      <td>2.095356</td>\n      <td>0.525228</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.875359</td>\n      <td>1.999668</td>\n      <td>0.559977</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.627453</td>\n      <td>1.819402</td>\n      <td>0.649333</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.430494</td>\n      <td>1.841276</td>\n      <td>0.679118</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.288909</td>\n      <td>1.796739</td>\n      <td>0.671224</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.193200</td>\n      <td>1.857251</td>\n      <td>0.688558</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.134485</td>\n      <td>1.752179</td>\n      <td>0.709554</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.096178</td>\n      <td>1.834427</td>\n      <td>0.721191</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.072396</td>\n      <td>1.826156</td>\n      <td>0.721680</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.059678</td>\n      <td>1.836468</td>\n      <td>0.726237</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.053572</td>\n      <td>1.843715</td>\n      <td>0.725993</td>\n      <td>00:01</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 01:40:00 - Questions\n\n* Can we use regularisation to make RNN params close to identity matrix?\n  * Will look at regularisation approaches.\n* Can you check if activations are exploding or vanishing?\n  * Yes. You can output activations of each layer with print statements.\n \n## 01:42:23 - Regularisation using Dropout\n\n* Dropout is basically deleting activations at random.\n  * By removing activations at random, no single activations can become too \"overspecialised\"\n* Dropout implementation:","metadata":{}},{"cell_type":"code","source":"class Dropout(nn.Module):\n    def __init__(self, p):\n        self.p = p\n        \n    def forward(self, x):\n        if not self.training:\n            return x\n        \n        mask = x.new(*x.shape).bernoulli_(1-p)\n        return x * mask.div_(1-p)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:32:36.120881Z","iopub.execute_input":"2021-08-28T01:32:36.121298Z","iopub.status.idle":"2021-08-28T01:32:36.128828Z","shell.execute_reply.started":"2021-08-28T01:32:36.121263Z","shell.execute_reply":"2021-08-28T01:32:36.127583Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"* A bermoulli random variable is a bunch of 1 and 0s with `1-p` probability of getting a 1.\n  * By multipying that by our input, we end up removing some layers.","metadata":{}},{"cell_type":"markdown","source":"## 01:47:16 - AR and TAR regularisation\n\n* Jeremy has only seen in RNNs.\n* AR (for activation regularisation)\n  * Similar to Weight Decay.\n  * Rather than adding a multiplier * sum of squares * weights.\n  * We add multiplier * sum of squares * activations.\n* TAR (for temporal activation regularisation).\n  * TAR is used to calculate the difference of activations between each layer.\n  \n## 01:49:09 - Weight tying","metadata":{}},{"cell_type":"markdown","source":"* Since predicting the next word is about converting activations to English words.\n * An embedding is about converting words to activations.\n \n* Hypothesis: since they're roughly the same idea, can't they use the same weight matrix?\n  * Yes! This appears to work in practice. ","metadata":{}},{"cell_type":"code","source":"class LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        \n        # This new line of code ensures that the weights of the embedding will always be the same as linear weights.\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        raw, h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out), raw, out\n    \n    def reset(self):\n        for h in self.h:\n            h.zero_()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:53:43.752850Z","iopub.execute_input":"2021-08-28T01:53:43.753310Z","iopub.status.idle":"2021-08-28T01:53:43.763578Z","shell.execute_reply.started":"2021-08-28T01:53:43.753271Z","shell.execute_reply":"2021-08-28T01:53:43.762206Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## 01:51:00 - TextLearner","metadata":{}},{"cell_type":"markdown","source":"* We pass in `RNNRegularizer` callback:","metadata":{}},{"cell_type":"code","source":"learn = Learner(\n    dls,\n    LMModel7(len(vocab), 64, 2, 0.5),\n    loss_func=CrossEntropyLossFlat(),\n    metrics=accuracy, cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:53:44.531720Z","iopub.execute_input":"2021-08-28T01:53:44.532142Z","iopub.status.idle":"2021-08-28T01:53:44.543347Z","shell.execute_reply.started":"2021-08-28T01:53:44.532107Z","shell.execute_reply":"2021-08-28T01:53:44.542172Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"* Or use the `TextLearner` which passes it for us:","metadata":{}},{"cell_type":"code","source":"learn = TextLearner(\n    dls,\n    LMModel7(len(vocab), 64, 2, 0.5),\n    loss_func=CrossEntropyLossFlat(),\n    metrics=accuracy\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:54:54.740827Z","iopub.execute_input":"2021-08-28T01:54:54.741270Z","iopub.status.idle":"2021-08-28T01:54:54.750734Z","shell.execute_reply.started":"2021-08-28T01:54:54.741236Z","shell.execute_reply":"2021-08-28T01:54:54.749594Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(15, 1e-2, wd=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T01:54:55.566840Z","iopub.execute_input":"2021-08-28T01:54:55.567264Z","iopub.status.idle":"2021-08-28T01:55:25.089644Z","shell.execute_reply.started":"2021-08-28T01:54:55.567230Z","shell.execute_reply":"2021-08-28T01:55:25.088353Z"},"trusted":true},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.583476</td>\n      <td>2.096658</td>\n      <td>0.452637</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.619627</td>\n      <td>1.360577</td>\n      <td>0.628581</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.874051</td>\n      <td>0.910055</td>\n      <td>0.776449</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.441865</td>\n      <td>0.803067</td>\n      <td>0.819824</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.226623</td>\n      <td>0.656291</td>\n      <td>0.852620</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.127504</td>\n      <td>0.559974</td>\n      <td>0.862549</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.077614</td>\n      <td>0.450280</td>\n      <td>0.887614</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.056029</td>\n      <td>0.559095</td>\n      <td>0.874430</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.038954</td>\n      <td>0.491778</td>\n      <td>0.877441</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.030191</td>\n      <td>0.524026</td>\n      <td>0.879964</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.024196</td>\n      <td>0.501803</td>\n      <td>0.877767</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.020231</td>\n      <td>0.504942</td>\n      <td>0.877686</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.016885</td>\n      <td>0.424204</td>\n      <td>0.889811</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.014706</td>\n      <td>0.461084</td>\n      <td>0.884603</td>\n      <td>00:01</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.013446</td>\n      <td>0.466097</td>\n      <td>0.884603</td>\n      <td>00:01</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"* We've now reproduced everything in AWD LSTM, which was state of the art a few years ago.","metadata":{}},{"cell_type":"markdown","source":"## 01:52:48 - Conclusions\n\n* Go idea to connect with other people in your community or forum who are along the learning journey.","metadata":{}}]}