@misc{3blue1brownCrossProductChapterEssence2016,
  title = {Cross Products \textbar{} {{Chapter}} 10, {{Essence}} of Linear Algebra},
  author = {{3Blue1Brown}},
  year = 2016,
  month = sep
}

@misc{3blue1brownVectorsChapter2Essence2016,
  title = {Linear Combinations, Span, and Basis Vectors},
  author = {{3Blue1Brown}},
  year = 2016,
  month = aug
}

@misc{3blue1brownVectorsChapter3Essence2016,
  title = {Linear Transformations and Matrices},
  author = {{3Blue1Brown}},
  year = 2016,
  month = aug
}

@misc{3blue1brownVectorsChapter6Essence2016,
  title = {The Determinant},
  author = {{3Blue1Brown}},
  year = 2016,
  month = aug
}

@misc{3blue1brownVectorsChapterEssence2016,
  title = {Vectors \textbar{} {{Chapter}} 1, {{Essence}} of Linear Algebra},
  author = {{3Blue1Brown}},
  year = 2016,
  month = aug
}

@misc{agrawalRotatingImage,
  title = {Rotating Image by Any Angle Using Only {{NumPy}}},
  author = {Agrawal, Gautam},
  year = 202,
  month = sep
}

@book{ahrensHowTakeSmart2017,
  title = {How to Take Smart Notes: {{One}} Simple Technique to Boost Writing, Learning and Thinking: {{For}} Students, Academics and Nonfiction Book Writers},
  shorttitle = {How to Take Smart Notes},
  author = {Ahrens, S{\"o}nke},
  year = 2017,
  publisher = {CreateSpace},
  address = {North Charleston, SC},
  isbn = {978-1-5428-6650-7},
  langid = {english}
}

@misc{behrouzNestedLearningIllusion2025,
  title = {Nested {{Learning}}: {{The Illusion}} of {{Deep Learning Architectures}}},
  shorttitle = {Nested {{Learning}}},
  author = {Behrouz, Ali and Razaviyayn, Meisam and Zhong, Peilin and Mirrokni, Vahab},
  year = 2025,
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2512.24695},
  urldate = {2026-01-18},
  abstract = {Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/lex/code/zotero/storage/UB2H9M96/Behrouz et al. - 2025 - Nested Learning The Illusion of Deep Learning Architectures.pdf}
}

@article{CrossEntropy2021,
  title = {Cross Entropy},
  author = {{Wikipedia}},
  year = 2021,
  month = jul,
  journal = {Wikipedia},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english}
}

@misc{dyeMathematicsMachineLearning,
  title = {Mathematics for {{Machine Learning}}: {{Linear Algebra}} - {{Home}}},
  shorttitle = {Mathematics for {{Machine Learning}}},
  author = {Dye, David and Cooper, Sam and Page, Freddie},
  year = 2018,
  abstract = {In this course on Linear Algebra we look at what linear algebra is and how it relates to vectors and matrices. Then we look through what vectors and matrices are and how to work with them, including the knotty problem of eigenvalues and ...},
  langid = {english}
}

@misc{fowlerTestPyramid2012,
  title = {{{TestPyramid}}},
  author = {Fowler, Martin},
  year = 2012,
  month = may
}

@misc{foxMachineLearningClassification,
  title = {Machine {{Learning}}: {{Classification}}},
  shorttitle = {Machine {{Learning}}},
  author = {Fox, Emily and Guestrin, Carlo},
  year = 2016,
  abstract = {Offered by University of Washington. Case Studies: Analyzing Sentiment \& Loan Default Prediction In our case study on analyzing sentiment, ... Enroll for free.},
  langid = {english}
}

@incollection{heathCurrentStatusDigital1998,
  title = {Current {{Status}} of the {{Digital Database}} for {{Screening Mammography}}},
  booktitle = {Digital {{Mammography}}},
  author = {Heath, M. and Bowyer, K. and Kopans, D. and Kegelmeyer, P. and Moore, R. and Chang, K. and Munishkumaran, S.},
  editor = {Viergever, Max A. and Karssemeijer, Nico and Thijssen, Martin and Hendriks, Jan and Van Erning, Leon},
  year = 1998,
  volume = {13},
  pages = {457--460},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-011-5318-8_75},
  urldate = {2026-01-10},
  isbn = {978-94-010-6234-3 978-94-011-5318-8},
  langid = {english}
}

@book{howardDeepLearningCoders2020,
  title = {Deep Learning for Coders with Fastai and {{PyTorch}}: {{AI}} Applications without a {{PhD}}},
  shorttitle = {Deep Learning for Coders with Fastai and {{PyTorch}}},
  author = {Howard, Jeremy and Gugger, Sylvain and Chintala, Soumith},
  year = 2020,
  publisher = {O'Reilly Media, Inc.},
  address = {Sebastopol, California},
  abstract = {Deep learning has the reputation as an exclusive domain for math PhDs. Not so. With this book, programmers comfortable with Python will learn how to get started with deep learning right away. Using PyTorch and the fastai deep learning library, you'll learn how to train a model to accomplish a wide range of tasks-including computer vision, natural language processing, tabular data, and generative networks. At the same time, you'll dig progressively into deep learning theory so that by the end of the book you'll have a complete understanding of the math behind the library's functions.},
  isbn = {978-1-4920-4552-6},
  langid = {english}
}

@misc{khanacademylab3x3Determinant,
  title = {3 x 3 Determinant},
  author = {{Khan Academy Labs}},
  year = 2009,
  month = nov
}

@misc{khanacademylabCrossProductIntroduction,
  title = {Cross Product Introduction},
  author = {{Khan Academy Labs}},
  year = 2009,
  month = oct
}

@misc{khanacademylabIntroMatrixInverses,
  title = {Introduction to Matrix Inverses},
  author = {{Khan Academy Labs}},
  year = 2008,
  month = jun
}

@misc{khanacademylabsInformationEntropyJourney,
  title = {Information Entropy \textbar{} {{Journey}} into Information Theory \textbar{} {{Computer Science}} \textbar{} {{Khan Academy}}},
  author = {{Khan Academy Labs}},
  year = 2014,
  month = apr
}

@misc{khanacademylabsLawCosinesSolving,
  title = {Law of Cosines},
  author = {{Khan Academy Labs}},
  year = 2013,
  month = aug
}

@book{khorikovUnitTestingPrinciples2020,
  title = {Unit Testing: {{Principles}}, Practices, and Patterns},
  shorttitle = {Unit Testing},
  author = {Khorikov, Vladimir},
  year = 2020,
  publisher = {Manning},
  address = {Shelter Island, NY},
  isbn = {978-1-61729-627-7},
  lccn = {TK5105.888 .K46 2020},
  keywords = {Testing,User interfaces (Computer systems),Web sites}
}

@article{leeCuratedMammographyData2017a,
  title = {A Curated Mammography Data Set for Use in Computer-Aided Detection and Diagnosis Research},
  author = {Lee, Rebecca Sawyer and Gimenez, Francisco and Hoogi, Assaf and Miyake, Kanae Kawai and Gorovoy, Mia and Rubin, Daniel L.},
  year = 2017,
  month = dec,
  journal = {Scientific Data},
  volume = {4},
  number = {1},
  pages = {170177},
  issn = {2052-4463},
  doi = {10.1038/sdata.2017.177},
  urldate = {2026-01-10},
  abstract = {Abstract             Published research results are difficult to replicate due to the lack of a standard evaluation data set in the area of decision support systems in mammography; most computer-aided diagnosis (CADx) and detection (CADe) algorithms for breast cancer in mammography are evaluated on private data sets or on unspecified subsets of public databases. This causes an inability to directly compare the performance of methods or to replicate prior results. We seek to resolve this substantial challenge by releasing an updated and standardized version of the Digital Database for Screening Mammography (DDSM) for evaluation of future CADx and CADe systems (sometimes referred to generally as CAD) research in mammography. Our data set, the CBIS-DDSM (Curated Breast Imaging Subset of DDSM), includes decompressed images, data selection and curation by trained mammographers, updated mass segmentation and bounding boxes, and pathologic diagnosis for training data, formatted similarly to modern computer vision data sets. The data set contains 753 calcification cases and 891 mass cases, providing a data-set size capable of analyzing decision support systems in mammography.},
  langid = {english},
  file = {/Users/lex/code/zotero/storage/NZG85QX2/Lee et al. - 2017 - A curated mammography data set for use in computer-aided detection and diagnosis research.pdf}
}

@misc{leeHowCorrectlyReport2025,
  title = {How to {{Correctly Report LLM-as-a-Judge Evaluations}}},
  author = {Lee, Chungpa and Zeng, Thomas and Jeong, Jongwon and Sohn, Jy-yong and Lee, Kangwook},
  year = 2025,
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2511.21140},
  urldate = {2025-11-28},
  abstract = {Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and typically assume exact knowledge of the model's specificity and sensitivity. Furthermore, in general we only have estimates of these values and it is not well known how to properly construct confidence intervals using only estimates. This work presents a simple plug-in framework that corrects such bias and constructs confidence intervals reflecting uncertainty from both test and calibration dataset, enabling practical and statistically sound LLM-based evaluation. Additionally, to reduce uncertainty in the accuracy estimate, we introduce an adaptive algorithm that efficiently allocates calibration sample sizes.},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  keywords = {Applications (stat.AP),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/Users/lex/code/zotero/storage/ASUXTFHR/Lee et al. - 2025 - How to Correctly Report LLM-as-a-Judge Evaluations.pdf}
}

@misc{linuxManPagesFork2,
  title = {Fork(2)},
  author = {man {pages}, Linux},
  year = 2021,
  month = aug
}

@book{perkinsMakingLearningWhole2010,
  title = {Making {{Learning Whole}}: {{How Seven Principles}} of {{Teaching Can Transform Education}}},
  shorttitle = {Making {{Learning Whole}}},
  author = {Perkins, David N and Ebrary, Inc},
  year = 2010,
  publisher = {Jossey Bass Inc},
  langid = {english}
}

@article{ramamoorthyTestingLargeSoftware1975,
  title = {Testing Large Software with Automated Software Evaluation Systems},
  author = {Ramamoorthy, C. V. and Ho, S. F.},
  year = 1975,
  month = jun,
  journal = {ACM SIGPLAN Notices},
  volume = {10},
  number = {6},
  pages = {382--394},
  issn = {0362-1340, 1558-1160},
  doi = {10.1145/390016.808461},
  urldate = {2026-01-26},
  abstract = {In the past few years, research has been actively carried out in an attempt to improve the quality and reliability of large scale software systems. Although progress has been made on the formal proof of program correctness, proving large scale software systems correct by formal proof is still many years away. Automated software tools have been found to be valuable in improving software reliability and attacking the high cost of software systems. This paper attempts to describe some main features of automated software tools and some software evaluation systems that are currently available.},
  langid = {english},
  file = {/Users/lex/code/zotero/storage/YKAVDP7H/Ramamoorthy and Ho - 1975 - Testing large software with automated software evaluation systems.pdf}
}

@misc{robloxBodyMover,
  title = {Vector3},
  author = {{Roblox}},
  year = 2021,
  month = jan
}

@misc{robloxVector3,
  title = {Vector3},
  author = {{Roblox}},
  year = 2021,
  month = aug
}

@book{schellArtGameDesign2015a,
  title = {The Art of Game Design: A Book of Lenses},
  shorttitle = {The Art of Game Design},
  author = {Schell, Jesse},
  year = 2015,
  edition = {Second edition},
  publisher = {CRC Press},
  address = {Boca Raton},
  isbn = {978-1-4665-9864-5},
  lccn = {QA76.76.C672 S34 2015},
  keywords = {Computer games,Design}
}

@article{thomasProblemMetricsFundamental2020,
  title = {The {{Problem}} with {{Metrics}} Is a {{Fundamental Problem}} for {{AI}}},
  author = {Thomas, Rachel and Uminsky, David},
  year = 2020,
  month = feb,
  journal = {arXiv:2002.08512 [cs]},
  eprint = {2002.08512},
  primaryclass = {cs},
  abstract = {Optimizing a given metric is a central aspect of most current AI approaches, yet overemphasizing metrics leads to manipulation, gaming, a myopic focus on short-term goals, and other unexpected negative consequences. This poses a fundamental contradiction for AI development. Through a series of real-world case studies, we look at various aspects of where metrics go wrong in practice and aspects of how our online environment and current business practices are exacerbating these failures. Finally, we propose a framework towards mitigating the harms caused by overemphasis of metrics within AI by: (1) using a slate of metrics to get a fuller and more nuanced picture, (2) combining metrics with qualitative accounts, and (3) involving a range of stakeholders, including those who will be most impacted.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society}
}

@article{wikipediaOrphanProcess,
  title = {Orphan Process},
  author = {{Wikipedia}},
  year = 2021,
  month = apr,
  journal = {Wikipedia},
  langid = {english}
}
